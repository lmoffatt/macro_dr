_Bueno, acá estoy caminando de nuevo por la 9 de julio, hay bastante tránsito, espero que esto se pueda escuchar bien. La verdad que no sé bien qué quiero decir, la idea es un poco planificando mis próximos trabajos, o sea, qué es lo que puedo publicar en los próximos meses. En principio tengo dos colaboraciones ya yendo, una es con Gustavo Pierdominici y la otra es con Cecilia Bousat. Con Gustavo, Gustavo le planteé hacer un modelo donde haya interacción explícita entre la rotación de subunidades, que la rotación de una subunidad depende del estado rotacional de las subunidades contiguas, es decir, cada una de las subunidades rotadas aporta un equilibrio hacia la rotación de esa subunidad. Eso es una f continuaci de nuestro paper habr que ver si hay algo m que pueda surgir de las simulaciones de Gustavo que puedan aportar a eso, ¿no? Alguna otra sutileza más. O algo que represente claramente los resultados que él sacó. Eso es una cosa. Después con Cecilia, bueno, estoy abierto a ver qué es lo que ella le parece que pueda yo aportar con mis tecnologías para los intereses de ella y muy abierto. Pero en principio lo que podemos buscar es ver si puede aplicarse modelos alostéricos a receptores CIS-LU. Bien, esas serían las dos cosas que tengo, digamos. Ahora, para lo primero, desde el punto de vista del algoritmo, lo que tengo que tener es lo mismo que tenía antes, nada diferente, solamente que funcionando y con cierta fe en que los resultados sean buenos._
 -- Transcrito por zapia.com, tu IA personal
 _Bueno, entonces en principio lo que tengo que tener funcionando es el algoritmo como lo tenía antes del PP pero con un poco más de sentirme cómodo con ese algoritmo. En particular que los tests funcionen a partir de la interfaz de comandos. O sea que a partir de la interfaz de comandos yo pueda hacer test que indiquen que el algoritmo funciona. Entonces la pregunta es ¿cuáles son esos test? Bueno, el primer test es verificar que la simulación de corrientes hechas por el algoritmo corresponde. Y después la otra es un test respecto de la validez de la función de likelihood. ese test de la validez de la función de likelihood lo hago a partir de la generación de muchas muestras usando la simulación de un esquema cinético y la idea es que la esperanza de la covarianza del score tiene que ser igual a la esperanza de la informaci de matriz de Fischer Eso, digamos, y lo más importante es que evaluado eso en los valores de parámetros que corresponden a la distribución simulada, la esperanza del score tiene que ser cero y tengo que poder hacer un cero con un test basado en la información de Fisher, es decir que sea, no va a ser exactamente cero pero digamos cero más menos digamos, el entorno de error de la función de Fisher. Y además el tema este de que la esperanza, la covarianza, perdón, la covarianza del score tiene que ser igual a la esperanza de la matriz de información de Fisher._
 -- Transcrito por zapia.com, tu IA personal
 _Bueno, finalmente me queda el tema, con eso verificaría que la función del ARK-HUD funciona. Luego me queda el tema de cómo yo verifico que la implementación de la estimación de la evidencia es correcta. Bueno, ahí cómo hacemos. ¿cómo hacemos ahí? bueno, ahí habría que en principio se podría hacer una función de una matriz de confusión respecto de de poder simular experimentos con un modelo y después fitearlos con varios modelos incluyendo el modelo en el cual fue simulado y que pueda distinguir eso de otros modelos. O sea, esa sería la prueba de fuego de este algoritmo. Bueno, eso lo podemos hacer, digamos, para condiciones que sean más simples. O sea, modelos que converjan rápido o que sean relativamente simples. los podemos probar por lo menos con modelos lineales. Los otros podemos hacer con modelos bastante complejos, sin mucha historia._
 -- Transcrito por zapia.com, tu IA personal
 _Entonces esto sería un tercer paper, una colaboración conmigo mismo, o sea con nadie más, que lo que haga básicamente sea explorar todas las cosas, o sea publicar todos los controles, todas las pruebas que yo hago para verificar que el algoritmo es el correcto. Eso sería un paper largo y tedioso que publicaré en algún lugar. En principio podría ser en BioArchives y después lo mando a algún otro lado. Pero que sería, digamos, como una verificación bastante formal de cómo funciona el algoritmo y viendo los distintos detalles. O sea, ahí podría ponerme un poquito denso, o sea, un poquito más sistemático y tratar de ir explorando las distintas cosas, o sea, tratando de... no explorar las distintas cosas, es decir, a ver cómo depende, por ejemplo, qué sé yo, el error de los datos, o sea, por ejemplo, a ver, a medida que yo aumento el número de puntos, cómo mejora la resolución del algoritmo en cuanto a los parámetros cinéticos en qué punto, por ejemplo, si yo tengo muy pocos datos no voy a poder distinguir entre modelos entonces puede ser que un modelo más simple que incorrecto fitea los datos y otro correcto no eso podr ocurrir tranquilamente o sea tiene que haber circunstancias donde eso ocurra y eso ser interesante explorarlo o sea eso ser como otro tipo de preguntas que se podrían hacer, claramente o también, por ejemplo, el otro tema muy importante es la likelihood distintos algoritmos de likelihood que tengo, cuáles son las condiciones en las cuales las aproximaciones son buenas eso también es un tema muy importante, es decir yo lo que tendría que ver es los distintos, las tres funciones importantes de macro R que serían simulation, likelihood y evidence ver digamos en qué condiciones funcionan, es decir claro, para eso es ver los tests o sea, tener un valorado un test y ver, bueno, ese test ver en qué circunstancias se cumplen, en qué circunstancias no se cumplen y también la velocidad, ¿no? ¿De qué depende la velocidad del test? Como para tener una idea del ámbito donde esto funciona. O sea, eso me parece que sería muy piola hacerlo al paralelo al trabajo con Gustavo. Es decir, mi plan tiene que ser tener algo más o menos que funcione rápido. O sea, en realidad podría empezar con esta segunda parte, es decir, tener los test, una vez que tengo los test de simulación, test de slide y test de evidencia, ya puedo empezar a hacer experimentos, entonces empiezo a hacer experimentos con modelos simples, y al mismo tiempo que estoy haciendo esos experimentos, voy pensando los modelos que tengo que fitear para Gustavo. Y mientras tanto hablo con Cecilia para ver, bueno, también lo mismo, o no sé a qué modelos ella necesita fitear o queda tostía._
 -- Transcrito por zapia.com, tu IA personal
 _Entonces volviendo a esto del tercer paper, que sería el segundo paper en realidad, que sería, digamos, una presentación formal de macro R, o sea, ver macro R y macro IR, en qué condiciones de número de datos, etc., de puntos, funciona, o sea, cómo vos podés diferenciar entre modelos de acuerdo a la cantidad de datos que tenés, etc., tratando también de ver el tema de la resolución temporal. Y quizás ahí también podemos meter con cumulative evidence, que eso sería un tercer paper, porque me parece que eso está separado. O sea, yo lo de cumulative evidence lo dejaría como para un tercer paper._
 -- Transcrito por zapia.com, tu IA personal
 _Bueno, a ver, ¿cuáles serían los comandos que tiene que tener mi sistema? Bueno, los comandos que tenía eran, primero que nada, evidencia. La evidencia está basada en la evidencia de un modelo para una distribución de parámetros, para unos datos y con algún algoritmo de evidencia y algoritmo de likelihood. o sea tenemos todos esos condimentos bien, después tenemos la función de likelihood que tiene todas las evidencias menos el algoritmo de evidencia o sea tiene un modelo, tiene los priors del modelo, los datos y el algoritmo de likelihood después tenemos la simulación que tiene el algoritmo de simulaci el modelo y los datos y un par no tiene par lo que no tiene es la se llama Los priors de par Despu podr inventar una que sea un tipo sampling No evidencia, sino sampling, que sería como el posterior, ¿no? Algo así como el posterior, habría que ver cuál es el nombre, que tendría que tener todos los mismos que la evidencia, menos el algoritmo de evidencia, tendría que tener un algoritmo de MSM, de Monte Carlo Markov Chain. Y después podríamos tener un algoritmo de idealización de datos, o sea que transformes un experimento en una submuestra de experimentos. Después tendríamos también una de construcción de modelos y una de construcción de distribuciones a priori. Y después una definición de experimentos. Esas son todas las funciones que tengo que definir._
 -- Transcrito por zapia.com, tu IA personal
 _Entonces, con eso todo definido, empiezo a pensar qué cosas me quedarían a mí por definir. Bueno, digamos, lo más controversial, entre comillas, serían, bueno, los datos y los modelos, o sea, cómo definir los datos, o sea, cómo definir el experimento. porque claro esto tendría que estar basado en los experimentos originales es decir los records de de action binary files o algo así y a partir de eso es armar los experimentos eso sería, pero claro eso tendría que ser una versión 2.0 una primera versión tomaría los datos tal cual los tengo ahora bien eso respecto de los datos, respecto del modelo también lo mismo, o sea, tendría que tener primero los modelos como los tengo ahora, o poder definir los modelos a partir de la, se llama la l de comandos o sea tener un DSL es decir un lenguaje espec de dominio espec para definir modelos, también un lenguaje de dominio específico para definir experimentos y después tendríamos las distribuciones a priori de los parámetros y los algoritmos diferentes que tenemos que definir, el algoritmo de simulación, el algoritmo de likelihood, el algoritmo de evidencia y el algoritmo de sampleado. Y luego tendríamos, la otra cosa es el algoritmo de derivada, o sea, de cómo tomar la derivada, que tenemos que tomar la derivada de la likelihood y para poder samplear el score y después lo que sería también la ficha Information Matrix, que eso lo necesitamos todo para poder hacer los tests, si no, no podemos hacer los tests de que funcione la likelihood._
 -- Transcrito por zapia.com, tu IA personal
 _Bueno, y acá me quedaría por definir si hay alguna manera de testear los algoritmos de likelihood que no involucren al score. Es decir, ¿puedo testear la likelihood sin score? Y la respuesta es que en principio no. O sea, podría también calcular la likelihood esperada como otra variable más, que tendría que ser otro comando más, la likelihood esperada. ¿Qué más puede ser? Y claro, ahí no sé, bueno, tendría la... Si tengo derivadas, puedo tener la derivada de la likelihood esperada, que no sé qué carajo significa. pero digamos, el problema con la ley cricudesperada es que eso ya lo vi que no te diferencia digamos, una institución que anda de una que no anda pero bueno no s no no no se me ocurre pensar alguna forma de d testeo de la likelihood está yo me acuerdo que bueno pensé mucho en el tema de distribución que vos este de probabilidad y sampleado no este pero que si vos tenés una likelihood y un sampleado vos podrías ver que una corresponde a la otra bueno eso todo eso tiene que ver con el tema que yo ya pensé bastante que era el tema de la likelihood y la la probabilidad y el sampling como como inversas y eso bueno no llegué a ningún puerto con eso, estuvimos mucho tiempo pensándolo y no llegamos a nada_
 -- Transcrito por zapia.com, tu IA personal
 _Bueno, entonces quedaría definido el plan. Una vez que tenga toda esta serie de comandos, que ya hablé más o menos todos definidos, entonces me quedarían experimentos, entre comillas, para hacer que corresponderían a algún tipo de paper. que eso tendríamos que definirlo después entonces el paper sería una serie de experimentos para hacer con estos comandos y una vez que se hacen los experimentos se hacen las figuras y se escribe el paper ahora vamos a tratar de adelantarnos un poquito y pensar cuáles serían esos experimentos Bueno, fácil es en principio pensar especialmente en lo que definen a los distintos parámetros que definen los algoritmos, Sí, los algoritmos, el algoritmo de la EIC, el algoritmo de la evidencia, el de sampleo, el de los dos sampleos, el de posteriori y el de sampleo de simulación, tendría que un poco trabajar en qué rangos de esos algoritmos las cosas funcionan. Y tambi el tema de como un dato de cada comando es qu precisi se tiene o sea tendr que haber un concepto de error de error en cuanto a apartamiento del valor óptimo o ideal o verdadero, y el de tiempo, ¿no? O sea, porque la idea es con esas dos cosas, la combinación de tiempo, de cálculo y precisión, es, digamos, se puede optimizar los cálculos para que, dado que uno tiene la cantidad determinada de tiempo de cómputo, qué precisión se puede llegar. Que eso sería un poco, uno podría plantearlo como uno de los objetivos del trabajo, es como tiempo y precisión en el cálculo y datos. O sea, vos tendrías cuánto tiempo de cálculo, qué precisión querés obtener en cuanto a los parámetros cinéticos o de poder diferenciar modelos, o sea, cuánta resolución, cuánto tiempo de cálculo y cuántos datos mismos necesitas para alcanzar todo eso. y por ahí posiblemente que simplificación de los datos también, pero ahí ya en ese último tema entraríamos en el cumulative evidence que eso sería para mí un nuevo otro paper_

