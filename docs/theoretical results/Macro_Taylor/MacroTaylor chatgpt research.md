
# Channel-Aware Laplace Approximation for the Posterior over $(p)$

This note collects the various derivations into a single coherent document.

---

## 0. Setup and Notation

We observe a scalar current $y$ generated by a population of $N_{\text{ch}}$ channels.
The parameter vector is $p \in \mathbb{R}^d$.

- **Mean current (per channel):** $\gamma \in \mathbb{R}^d$
- **Noise variance contribution (per channel):** $\sigma^2 \in \mathbb{R}^d$
- **Additive baseline noise variance:** $\epsilon^2 > 0$

Given $p$, define:

- **Residual**
  $$
  \delta \;=\; y - N_{\text{ch}} \,(p \cdot \gamma)
  $$
- **Total (heteroscedastic) variance**
  $$
  V \;=\; \epsilon^2 + N_{\text{ch}} \,(p \cdot \sigma^2)
  $$

The (unnormalised) negative log-posterior **without** the usual factor $1/2$ is written as an “energy”
$$
E(p) \;=\; M(p) + P(p),
$$
with

- **Likelihood term**
  $$
  M(p) \;=\; \frac{\delta^2}{V}
  $$
- **Prior term**
  $$
  P(p) \;=\; (p - \mu)^\top \Bigl(N_{\text{ch}} \,\Sigma_p^{-1}\Bigr)\,(p - \mu),
  $$
  where $\mu$ is the prior mean and $\Sigma_p$ is a *reference* covariance (for $N_{\text{ch}} = 1$).
  The corresponding prior covariance for general $N_{\text{ch}}$ is
  $$
  \Sigma_{\text{prior}} \;=\; \frac{1}{N_{\text{ch}}}\,\Sigma_p.
  $$

The **exact** posterior is
$$
p(p \mid y) \;\propto\; \exp\!\bigl(-\tfrac{1}{2} E(p)\bigr).
$$

We denote by

- $\mathbf{g} = \nabla E(p)$ the gradient,
- $\mathbf{H} = \nabla^2 E(p)$ the Hessian,

evaluated at some expansion point $p_0$ (typically $p_0 = \mu$).

In what follows, whenever $(\delta, V)$ appear inside the Gaussian approximation, they are understood to be evaluated at the expansion point $p_0$.

---

## 1. The Approximation We Make

We use a **second-order Taylor (Laplace) approximation** to the log posterior around an expansion point $p_0$:
$$
E(p) \;\approx\; E(p_0)
\;+\; \mathbf{g}_0^\top (p - p_0)
\;+\; \frac{1}{2} (p - p_0)^\top \mathbf{H}_0 (p - p_0),
$$
where
$$
\mathbf{g}_0 \;=\; \nabla E(p_0),
\qquad
\mathbf{H}_0 \;=\; \nabla^2 E(p)\big|_{p = p_0}.
$$

Plugging this into the posterior
$$
p(p \mid y) \;\propto\; \exp\!\bigl(-\tfrac{1}{2} E(p)\bigr)
$$
gives a **Gaussian approximation**:
$$
p(p \mid y) \;\approx\; \mathcal{N}\bigl(p \,\big|\, p_{\text{post}}, \Sigma_{\text{post}}\bigr),
$$
with

- **Approximate covariance**
  $$
  \Sigma_{\text{post}} \;\approx\; 2\,\mathbf{H}_0^{-1},
  $$
- **Approximate mean (one Newton step from $p_0$)**
  $$
  p_{\text{post}} \;\approx\; p_0 - \mathbf{H}_0^{-1}\,\mathbf{g}_0.
  $$

All the non-trivial work is in computing $\mathbf{H}_0^{-1}$ efficiently and in simplifying $p_{\text{post}}$. Because the likelihood contribution is rank-1 (after some algebra), we can invert $\mathbf{H}_0$ in **closed form** using the Sherman–Morrison formula.

---

## 2. Result: Posterior Covariance and Posterior Mean

### 2.1 Definitions at the Expansion Point

Pick an expansion point $p_0$ (for the final “nice” mean expression we choose $p_0 = \mu$).
At $p_0$, define:

- Residual and variance:
  $$
  \delta \;=\; y - N_{\text{ch}} \,(p_0 \cdot \gamma),
  \qquad
  V \;=\; \epsilon^2 + N_{\text{ch}} \,(p_0 \cdot \sigma^2).
  $$

- Scalar
  $$
  \alpha \;=\; \frac{\delta}{V}.
  $$

- “Effective” vector combining mean and variance contributions:
  $$
  v \;=\; \gamma + \alpha\,\sigma^2
  \;=\; \gamma + \frac{\delta}{V}\,\sigma^2.
  $$

- Scalar
  $$
  s \;=\; v^\top \Sigma_p\,v.
  $$

Everything below is evaluated at this $p_0$.

---

### 2.2 Posterior Covariance via Sherman–Morrison

The Laplace covariance is
$$
\Sigma_{\text{post}} \;\approx\; 2\,\mathbf{H}_0^{-1}.
$$

Using the Sherman–Morrison formula (see derivation in §4), the Hessian can be written as a rank-1 update of the prior precision, and its inverse can be obtained in closed form. The final expression for the **posterior covariance** is
$$
\boxed{
\Sigma_{\text{post}}
\;\approx\;
\frac{1}{N_{\text{ch}}}\,\Sigma_p
\;-\;
\frac{1}{V + N_{\text{ch}}\,s}\;
\Sigma_p\, v v^\top \Sigma_p
}
$$
with
$$
v \;=\; \gamma + \frac{\delta}{V}\,\sigma^2,
\qquad
s \;=\; v^\top \Sigma_p v,
\qquad
\delta \;=\; y - N_{\text{ch}} (p_0 \cdot \gamma),
\qquad
V \;=\; \epsilon^2 + N_{\text{ch}} (p_0 \cdot \sigma^2).
$$

Interpretation:

- The first term $\dfrac{1}{N_{\text{ch}}}\Sigma_p$ is exactly the **prior covariance**.
- The second term is a **rank-1 reduction** along the direction $v$, whose strength is controlled by
  $V + N_{\text{ch}} s$, combining measurement noise $V$ and prior predictive variance $N_{\text{ch}} s$.

---

### 2.3 Posterior Mean

In full generality, the Laplace mean is
$$
p_{\text{post}}
\;\approx\;
p_0 - \mathbf{g}_0\, \mathbf{H}_0^{-1}
\;=\;
p_0 - \frac{1}{2}\,\mathbf{g}_0\,\Sigma_{\text{post}}
$$
since $\Sigma_{\text{post}} = 2\,\mathbf{H}_0^{-1}$.

From the gradient (derived in §4.1),
$$
\mathbf{g}_0
\;=\;
2 N_{\text{ch}}\,(p_0 - \mu)\, \Sigma_p^{-1} 
\;-\;
N_{\text{ch}}
\left(
\frac{2\delta}{V}\,\gamma ^{\top} + \frac{\delta^2}{V^2}\,{\sigma^2}^{\top}
\right),
$$
or, in terms of $v$,
$$
\mathbf{g}_0
\;=\;
2 N_{\text{ch}}\,(p_0 - \mu)
\,\Sigma_p^{-1} \;-\;
N_{\text{ch}}\,\frac{\delta}{V}\,\bigl(\gamma^{\top} + v^{\top}\bigr).
$$

So the **general** Laplace mean update can be written as
$$
\boxed{
p_{\text{post}} \;\approx\;
p_0 - \frac{1}{2}\,
\Bigl[
2 N_{\text{ch}}\,(p_0 - \mu)\,\Sigma_p^{-1} 
\;-\;
N_{\text{ch}}\,\frac{\delta}{V}\,\bigl(\gamma^{\top} + v^{\top}\bigr)
\Bigr]\,\Sigma_{\text{post}}
}
$$
with the same $(\delta, V, v)$ as above.

---

Let me do this algebraically and kill every explicit \(\Sigma_p^{-1}\).

From the “Channel-aware Laplace” note:

* Posterior covariance (2.2):

\[
\Sigma_{\text{post}}
\,=
\frac{1}{N_{\text{ch}}}\,\Sigma_p
\, -
\frac{1}{V + N_{\text{ch}} s}\,
\Sigma_p v v^\top \Sigma_p,
\qquad
s = v^\top \Sigma_p v,
\quad
v = \gamma + \frac{\delta}{V}\sigma^2.
\]

* General mean (2.3):

\[
p_{\text{post}} \approx
p_0 - \frac{1}{2}\,\Sigma_{\text{post}}
\Bigl[
2 N_{\text{ch}}\,\Sigma_p^{-1} (p_0 - \mu)
\, - N_{\text{ch}}\,\frac{\delta}{V}\,(\gamma + v)
\Bigr].
\]

Write \(N = N_{\text{ch}}\) and \(\Delta p := p_0 - \mu\). Then

\[
p_{\text{post}}
\,=
p_0 - \Sigma_{\text{post}} N \Sigma_p^{-1}\Delta p
\,+ \Sigma_{\text{post}}\frac{N\delta}{2V}(\gamma + v).
\tag{1}
\]

---

### 1. Cancel \(\Sigma_p^{-1}\) using the explicit \(\Sigma_{\text{post}}\)

First compute \(\Sigma_{\text{post}}\Sigma_p^{-1}\) from the closed form of \(\Sigma_{\text{post}}\):

\[
\begin{aligned}
\Sigma_{\text{post}}\Sigma_p^{-1}
\,&=
\Bigl(\tfrac{1}{N}\Sigma_p
\,-
\tfrac{1}{V+Ns}\Sigma_p v v^\top \Sigma_p\Bigr)\Sigma_p^{-1}\\[3pt]
\,&=
\tfrac{1}{N} \Sigma_p\Sigma_p^{-1}
\,-
\tfrac{1}{V+Ns}\Sigma_p v v^\top \underbrace{\Sigma_p\Sigma_p^{-1}}_{I}\\[3pt]
\,&=
\tfrac{1}{N} I - \frac{1}{V+Ns}\Sigma_p v v^\top.
\end{aligned}
\]

Therefore

\[
\Sigma_{\text{post}} N \Sigma_p^{-1}
\,=
I - \frac{N}{V+Ns}\Sigma_p v v^\top.
\]

Plug into the first term of (1):

\[
\begin{aligned}
\Sigma_{\text{post}} N \Sigma_p^{-1}\Delta p
\,&=
\Bigl(I - \frac{N}{V+Ns}\Sigma_p v v^\top\Bigr)\Delta p\\
\,&=
\Delta p - \frac{N}{V+Ns}\,\Sigma_p v \bigl(v^\top \Delta p\bigr).
\end{aligned}
\]

So (1) becomes

\[
\begin{aligned}
p_{\text{post}}
\,&=
p_0 - \Bigl[\Delta p - \frac{N}{V+Ns}\Sigma_p v (v^\top \Delta p)\Bigr]
\,+ \Sigma_{\text{post}}\frac{N\delta}{2V}(\gamma + v)\\[3pt]
\,&=
\underbrace{(p_0 - \Delta p)}_{=\mu}
\,+ \frac{N}{V+Ns}\Sigma_p v (v^\top\Delta p)
\,+ \Sigma_{\text{post}}\frac{N\delta}{2V}(\gamma + v).
\end{aligned}
\]

Thus the **general mean** can be written with no \(\Sigma_p^{-1}\):

\[
\boxed{
p_{\text{post}}
\,=
\mu
\,+ \frac{N_{\text{ch}}}{V + N_{\text{ch}} s}\,\Sigma_p v \bigl(v^\top (p_0 - \mu)\bigr)
\,+ \Sigma_{\text{post}}\frac{N_{\text{ch}}\delta}{2V}(\gamma + v)
}
\tag{2}
\]

with \(\Sigma_{\text{post}}\) itself given purely in terms of \(\Sigma_p\) in (2.2).

No explicit \(\Sigma_p^{-1}\) survives; it has been algebraically cancelled.

---

### 2. Fully expanded version (everything in terms of \(\Sigma_p\))

If you also expand the last term using the explicit \(\Sigma_{\text{post}}\),

\[
\begin{aligned}
\Sigma_{\text{post}}\frac{N\delta}{2V}(\gamma + v)
\,&=
\frac{N\delta}{2V}
\Bigl(\tfrac{1}{N}\Sigma_p
\,-
\tfrac{1}{V+Ns}\Sigma_p v v^\top \Sigma_p\Bigr)(\gamma + v)\\[3pt]
\,&=
\frac{\delta}{2V}\Sigma_p(\gamma + v)
\,-\frac{N\delta}{2V(V+Ns)}\,\Sigma_p v \Bigl(v^\top\Sigma_p(\gamma \,+ v)\Bigr).
\end{aligned}
\]

Plugging this into (2) gives a **completely expanded mean**:

\[
\boxed{
\begin{aligned}
p_{\text{post}}
\,&=
\mu
\,+ \frac{N_{\text{ch}}}{V + N_{\text{ch}} s}\,\Sigma_p v \bigl(v^\top (p_0 - \mu)\bigr)\\[3pt]
&\quad
\,+ \frac{\delta}{2V}\,\Sigma_p(\gamma + v)
\,- \frac{N_{\text{ch}}\delta}{2V(V + N_{\text{ch}} s)}\,
  \Sigma_p v \Bigl(v^\top\Sigma_p(\gamma + v)\Bigr).
\end{aligned}
}
\tag{3}
\]



### 2.4 Special Case: Expansion at the Prior Mean $(p_0 = \mu)$

If we expand around the prior mean $p_0 = \mu$, then $p_0 - \mu = 0$ and the gradient simplifies to
$$
\mathbf{g}_0
\;=\;
-\,N_{\text{ch}}\,\frac{\delta}{V}\,(\gamma + v),
$$
and the update becomes:
$$
\boxed{
p_{\text{post}}
\;\approx\;
\mu + \frac{N_{\text{ch}}\,\delta}{2V}\;\Sigma_{\text{post}}\,(\gamma + v)
}
$$
where:

- $\delta = y - N_{\text{ch}}\,(\mu \cdot \gamma)$,
- $V = \epsilon^2 + N_{\text{ch}}\,(\mu \cdot \sigma^2)$,
- $v = \gamma + \dfrac{\delta}{V}\,\sigma^2$,
- $\Sigma_{\text{post}}$ is the covariance from §2.2 evaluated with these $(\delta, V, v, s)$.

This shows:

- The posterior mean is the prior mean plus a correction
  proportional to the **posterior covariance** and the
  **effective direction** $(\gamma + v)$.
- The step size scales with the residual $\delta$, the channel count $N_{\text{ch}}$,
  and $1/V$ (effective inverse noise level).

---

## 3. Hint of the Derivation (High-Level Sketch)

Here is the “short story” of how the above formulas arise.

1. **Write down the energy.**  
   Use
   $$
   E(p)
   \;=\;
   \frac{\delta^2}{V}
   \;+\;
   (p - \mu)^\top (N_{\text{ch}}\Sigma_p^{-1})(p - \mu),
   $$
   with $\delta = y - N_{\text{ch}}(p \cdot \gamma)$ and
   $V = \epsilon^2 + N_{\text{ch}}(p \cdot \sigma^2)$.

2. **Compute the gradient and Hessian.**  
   Treat $M(p)=\delta^2/V$ using the quotient rule, noting that
   $\nabla \delta = -N_{\text{ch}}\gamma$ and $\nabla V = N_{\text{ch}}\sigma^2$.
   Add the prior contributions to obtain $\mathbf{g} = \nabla E$ and
   $\mathbf{H} = \nabla^2 E$.

3. **Observe that the Hessian is a rank-1 update.**  
   After some algebra the likelihood Hessian part can be written as
   a matrix proportional to
   $$
   \bigl(\gamma + \alpha\sigma^2\bigr)\bigl(\gamma + \alpha\sigma^2\bigr)^\top,
   \qquad \alpha = \delta/V.
   $$
   Thus
   $$
   \mathbf{H} \;=\;
   2 N_{\text{ch}}\Bigl[\Sigma_p^{-1} + u u^\top\Bigr],
   $$
   for a suitable vector $u$ (a rescaled version of $v$).

4. **Apply Sherman–Morrison to invert $\Sigma_p^{-1} + u u^\top$.**  
   Since this is a rank-1 update of the prior precision,
   we can use
   $$
   (A + u u^\top)^{-1}
   \;=\;
   A^{-1}
   \;-\;
   \frac{A^{-1} u u^\top A^{-1}}{1 + u^\top A^{-1} u}
   $$
   with $A = \Sigma_p^{-1}$, so $A^{-1} = \Sigma_p$.

5. **Relate $\mathbf{H}^{-1}$ to $\Sigma_{\text{post}}$.**  
   The Laplace covariance is $\Sigma_{\text{post}} = 2\,\mathbf{H}^{-1}$.
   Using the previous step and re-expressing in terms of
   $v = \gamma + (\delta/V)\sigma^2$ and
   $s = v^\top \Sigma_p v$, we obtain the closed-form
   covariance in §2.2.

6. **Use the Newton step for the mean.**  
   The mean is
   $$
   p_{\text{post}} \approx p_0 - \mathbf{H}_0^{-1}\mathbf{g}_0
   = p_0 - \frac{1}{2}\Sigma_{\text{post}}\mathbf{g}_0.
   $$
   Plugging the explicit $\mathbf{g}_0$ and choosing
   $p_0 = \mu$ gives the compact formula in §2.4.

---

## 4. Complete Derivation

### 4.1 Gradient and Hessian of the Energy

Recall:
$$
E(p) = M(p) + P(p),
\qquad
M(p) = \frac{\delta^2}{V},
\qquad
P(p) = (p - \mu)^\top (N_{\text{ch}}\Sigma_p^{-1})(p - \mu),
$$
with
$$
\delta = y - N_{\text{ch}}(p\cdot \gamma),
\qquad
V = \epsilon^2 + N_{\text{ch}}(p \cdot \sigma^2).
$$

#### 4.1.1 Prior Gradient and Hessian

The prior is quadratic in $p$, so:

- Gradient:
  $$
  \nabla P(p) = 2 N_{\text{ch}}\,\Sigma_p^{-1} (p - \mu).
  $$

- Hessian:
  $$
  \nabla^2 P(p) = 2 N_{\text{ch}}\,\Sigma_p^{-1}.
  $$

#### 4.1.2 Likelihood Gradient

For the likelihood term $M(p) = \delta^2/V$, we first note:

- $\delta$ and $V$ are linear in $p$, so their gradients are constant:
  $$
  \nabla \delta = -N_{\text{ch}}\,\gamma,
  \qquad
  \nabla V     = N_{\text{ch}}\,\sigma^2.
  $$

Using the quotient rule for scalars,
$$
M(p) = \frac{\delta^2}{V}
\quad\Rightarrow\quad
\nabla M(p)
\, = \frac{2\delta}{V}\,\nabla \delta
\;-\;
\frac{\delta^2}{V^2}\,\nabla V.
$$
Substitute $\nabla\delta$ and $\nabla V$:
$$
\nabla M(p)
\, = \frac{2\delta}{V}(-N_{\text{ch}}\gamma)
\;-\;
\frac{\delta^2}{V^2}(N_{\text{ch}}\sigma^2)
\, = -N_{\text{ch}}
\left(
\frac{2\delta}{V}\gamma + \frac{\delta^2}{V^2}\sigma^2
\right).
$$

So the **total gradient** is
$$
\boxed{
\mathbf{g}(p)
\, = \nabla E(p)
\, = 2 N_{\text{ch}}\,\Sigma_p^{-1}(p - \mu)
\;-\;
N_{\text{ch}}
  \left(
  \frac{2\delta}{V}\gamma + \frac{\delta^2}{V^2}\sigma^2
  \right)
}
$$

#### 4.1.3 Likelihood Hessian

We now differentiate $\nabla M(p)$ again.

Write
$$
\nabla M(p) = -N_{\text{ch}}\bigl(a\,\gamma + b\,\sigma^2\bigr),
$$
where
$$
a = \frac{2\delta}{V},
\qquad
b = \frac{\delta^2}{V^2}.
$$

Since $\gamma$ and $\sigma^2$ are constant vectors,
$$
\nabla^2 M(p)
\, = -N_{\text{ch}}\bigl[ (\nabla a)\,\gamma^\top + (\nabla b)\,(\sigma^2)^\top \bigr].
$$

We compute $\nabla a$ and $\nabla b$:

1. For $a = 2\delta/V$,
   $$
   \nabla a
   = 2\left(\frac{1}{V}\nabla \delta - \frac{\delta}{V^2}\nabla V\right)
   = 2\left(\frac{-N_{\text{ch}}\gamma}{V} - \frac{\delta}{V^2}N_{\text{ch}}\sigma^2\right)
   = -\frac{2N_{\text{ch}}}{V}\gamma - \frac{2N_{\text{ch}}\delta}{V^2}\sigma^2.
   $$

2. For $b = \delta^2/V^2$,
   $$
   \nabla b
   = 2\frac{\delta}{V^2}\nabla \delta - 2\frac{\delta^2}{V^3}\nabla V
   = 2\frac{\delta}{V^2}(-N_{\text{ch}}\gamma) - 2\frac{\delta^2}{V^3}(N_{\text{ch}}\sigma^2)
   = -\frac{2N_{\text{ch}}\delta}{V^2}\gamma - \frac{2N_{\text{ch}}\delta^2}{V^3}\sigma^2.
   $$

Therefore
$$
\nabla^2 M(p)
\, = -N_{\text{ch}}\Bigl[
\left(-\frac{2N_{\text{ch}}}{V}\gamma - \frac{2N_{\text{ch}}\delta}{V^2}\sigma^2\right)\gamma^\top
\, +
\left(-\frac{2N_{\text{ch}}\delta}{V^2}\gamma - \frac{2N_{\text{ch}}\delta^2}{V^3}\sigma^2\right)(\sigma^2)^\top
\Bigr].
$$

Distribute the minus sign and collect terms:
$$
\nabla^2 M(p)
\, = 2 N_{\text{ch}}^2
\left[
\frac{1}{V}\gamma\gamma^\top
\, +\frac{\delta}{V^2}\bigl(\gamma(\sigma^2)^\top + \sigma^2\gamma^\top\bigr)
\, +\frac{\delta^2}{V^3}\sigma^2 (\sigma^2)^\top
\right].
$$

Thus the **total Hessian** is
$$
\boxed{
\mathbf{H}(p)
\, = \nabla^2 E(p)
\, = 2 N_{\text{ch}}\,\Sigma_p^{-1}
\;+\;
2 N_{\text{ch}}^2
  \left[
  \frac{1}{V}\gamma\gamma^\top
  +\frac{\delta}{V^2}\bigl(\gamma(\sigma^2)^\top + \sigma^2\gamma^\top\bigr)
  +\frac{\delta^2}{V^3}\sigma^2(\sigma^2)^\top
  \right]
}
$$

All of this is **exact**; the approximation only enters when we freeze $(\delta, V)$ at the expansion point $p_0$ and use the Hessian there to build a Gaussian.

---

### 4.2 Factorising the Hessian as a Rank-1 Update

We now show that the big bracket in $\mathbf{H}$ can be written as a **single outer product**.

Define again
$$
\alpha = \frac{\delta}{V},
\qquad
v = \gamma + \alpha\,\sigma^2.
$$

Consider the outer product:
$$
v v^\top
\, = (\gamma + \alpha\sigma^2)(\gamma + \alpha\sigma^2)^\top
\, = \gamma\gamma^\top
\, + \alpha\bigl(\gamma(\sigma^2)^\top + \sigma^2\gamma^\top\bigr)
\, + \alpha^2 \sigma^2(\sigma^2)^\top.
$$

Multiply by $N_{\text{ch}}/V$:
$$
\frac{N_{\text{ch}}}{V} v v^\top
\, = \frac{N_{\text{ch}}}{V} \gamma\gamma^\top
\, + \frac{N_{\text{ch}}\delta}{V^2}\bigl(\gamma(\sigma^2)^\top + \sigma^2\gamma^\top\bigr)
\, + \frac{N_{\text{ch}}\delta^2}{V^3} \sigma^2(\sigma^2)^\top.
$$

This is exactly the likelihood Hessian bracket (inside the factor $2 N_{\text{ch}}^2$):
$$
2N_{\text{ch}}^2
\left[
\frac{1}{V}\gamma\gamma^\top
\, +\frac{\delta}{V^2}(\gamma(\sigma^2)^\top + \sigma^2\gamma^\top)
\, +\frac{\delta^2}{V^3}\sigma^2(\sigma^2)^\top
\right]
\, =
2N_{\text{ch}}^2 \frac{1}{V} v v^\top.
$$

Thus we can rewrite the Hessian as
$$
\mathbf{H}
\, = 2 N_{\text{ch}}\,\Sigma_p^{-1}
\, + 2 N_{\text{ch}}^2 \frac{1}{V} v v^\top
\, = 2 N_{\text{ch}}\left[
\Sigma_p^{-1} + \frac{N_{\text{ch}}}{V} v v^\top
\right].
$$

Define
$$
u = \sqrt{\frac{N_{\text{ch}}}{V}}\,v,
$$
so that
$$
u u^\top = \frac{N_{\text{ch}}}{V}\, v v^\top.
$$

Then
$$
\boxed{
\mathbf{H}
\, = 2 N_{\text{ch}}\bigl[\Sigma_p^{-1} + u u^\top\bigr].
}
$$

This is the key structural step:
$\mathbf{H}$ is a scaled version of a **rank-1 update** of the prior precision matrix $\Sigma_p^{-1}$.

---

### 4.3 Inverting the Hessian via Sherman–Morrison

The Sherman–Morrison formula for a rank-1 update says that, for an invertible $A$,
$$
(A + u u^\top)^{-1}
\, = A^{-1}
\, - \frac{A^{-1} u u^\top A^{-1}}{1 + u^\top A^{-1} u}.
$$

In our case:

- $A = \Sigma_p^{-1}$, so $A^{-1} = \Sigma_p$;
- the update is $u u^\top$.

Hence
$$
(\Sigma_p^{-1} + u u^\top)^{-1}
\, = \Sigma_p
\, - \frac{\Sigma_p u u^\top \Sigma_p}{1 + u^\top \Sigma_p u}.
$$

Since
$$
\mathbf{H} = 2 N_{\text{ch}}(\Sigma_p^{-1} + u u^\top),
$$
we have
$$
\mathbf{H}^{-1}
\, = \frac{1}{2 N_{\text{ch}}}
(\Sigma_p^{-1} + u u^\top)^{-1}
\, = \frac{1}{2 N_{\text{ch}}}
\left[
\Sigma_p
\, - \frac{\Sigma_p u u^\top \Sigma_p}{1 + u^\top \Sigma_p u}
\right].
$$

The Laplace covariance is
$$
\Sigma_{\text{post}} = 2\,\mathbf{H}^{-1}
\, = \frac{1}{N_{\text{ch}}}
\left[
\Sigma_p
\, - \frac{\Sigma_p u u^\top \Sigma_p}{1 + u^\top \Sigma_p u}
\right].
$$

Now express everything in terms of $v$:
$$
u = \sqrt{\frac{N_{\text{ch}}}{V}}\,v
\quad\Rightarrow\quad
u u^\top = \frac{N_{\text{ch}}}{V} v v^\top,
$$
and
$$
u^\top \Sigma_p u
\, = \frac{N_{\text{ch}}}{V} v^\top \Sigma_p v
\, = \frac{N_{\text{ch}}}{V} s,
\quad
s = v^\top \Sigma_p v.
$$

Therefore:

- Numerator:
  $$
  \Sigma_p u u^\top \Sigma_p
\,  = \Sigma_p \left(\frac{N_{\text{ch}}}{V} v v^\top\right)\Sigma_p
\,  = \frac{N_{\text{ch}}}{V} \Sigma_p v v^\top \Sigma_p.
  $$
- Denominator:
  $$
  1 + u^\top \Sigma_p u
\,  = 1 + \frac{N_{\text{ch}}}{V}s
\,  = \frac{V + N_{\text{ch}} s}{V}.
  $$

Plugging into $\Sigma_{\text{post}}$:
$$
\Sigma_{\text{post}}
\, = \frac{1}{N_{\text{ch}}}
\left[
\Sigma_p
\, - \frac{\frac{N_{\text{ch}}}{V}\Sigma_p v v^\top \Sigma_p}{\frac{V + N_{\text{ch}}s}{V}}
\right]
\, = \frac{1}{N_{\text{ch}}}
\left[
\Sigma_p
\, - \frac{N_{\text{ch}}\,\Sigma_p v v^\top \Sigma_p}{V + N_{\text{ch}}s}
\right].
$$

Distribute the factor $1/N_{\text{ch}}$:
$$
\boxed{
\Sigma_{\text{post}}
\;\approx\;
\frac{1}{N_{\text{ch}}}\,\Sigma_p
\;-\;
\frac{1}{V + N_{\text{ch}}s}\;
\Sigma_p\, v v^\top \Sigma_p
}
$$
with
$$
v = \gamma + \frac{\delta}{V}\sigma^2,
\qquad
s = v^\top \Sigma_p v.
$$

This reproduces the compact covariance formula in §2.2.

---

### 4.4 Newton Step and Posterior Mean

Recall the Laplace mean is obtained by one **Newton–Raphson** update from $p_0$:
$$
p_{\text{post}} \approx p_0 - \mathbf{H}_0^{-1}\mathbf{g}_0.
$$

We already have

- $\mathbf{H}_0^{-1}$ in closed form through $\Sigma_{\text{post}} = 2\mathbf{H}_0^{-1}$,
- $\mathbf{g}_0 = \nabla E(p_0)$:
  $$
  \mathbf{g}_0
\,  =
  2 N_{\text{ch}}\,\Sigma_p^{-1}(p_0 - \mu)
  \,- 
  N_{\text{ch}}
  \left(
  \frac{2\delta}{V}\gamma + \frac{\delta^2}{V^2}\sigma^2
  \right).
  $$

Using $v = \gamma + (\delta/V)\sigma^2$, we can rewrite
$$
\frac{2\delta}{V}\gamma + \frac{\delta^2}{V^2}\sigma^2
\, = \frac{\delta}{V}\left(2\gamma + \frac{\delta}{V}\sigma^2\right)
\, = \frac{\delta}{V}\bigl(\gamma + v\bigr),
$$
hence
$$
\mathbf{g}_0
\, =
2 N_{\text{ch}}\,\Sigma_p^{-1}(p_0 - \mu)
\;-\;
N_{\text{ch}}\,\frac{\delta}{V}(\gamma + v).
$$

Since $\Sigma_{\text{post}} = 2\mathbf{H}_0^{-1}$, we have
$$
\mathbf{H}_0^{-1} = \frac{1}{2}\Sigma_{\text{post}},
$$
so
$$
p_{\text{post}}
\approx
p_0 - \frac{1}{2}\Sigma_{\text{post}}\mathbf{g}_0.
$$

Substituting $\mathbf{g}_0$:
$$
p_{\text{post}}
\approx
p_0 - \frac{1}{2}\Sigma_{\text{post}}
\Bigl[
2 N_{\text{ch}}\,\Sigma_p^{-1}(p_0 - \mu)
\;-\;
N_{\text{ch}}\,\frac{\delta}{V}(\gamma + v)
\Bigr].
$$

That is the general expression in §2.3.

#### Special case $p_0 = \mu$

If we choose the expansion point to be the prior mean ($p_0 = \mu$), then:

- $p_0 - \mu = 0$,
- the gradient reduces to
  $$
  \mathbf{g}_0
\,  = -\,N_{\text{ch}}\,\frac{\delta}{V}(\gamma + v),
  $$
- and the update becomes
  $$
  p_{\text{post}}
  \approx
  \mu - \frac{1}{2}\Sigma_{\text{post}}
  \left[ -N_{\text{ch}}\,\frac{\delta}{V}(\gamma + v) \right]
\,  =
  \mu + \frac{N_{\text{ch}}\,\delta}{2V}\,\Sigma_{\text{post}}(\gamma + v).
  $$

Thus we recover the concise expression
$$
\boxed{
p_{\text{post}}
\;\approx\;
\mu + \frac{N_{\text{ch}}\,\delta}{2V}\,\Sigma_{\text{post}}(\gamma + v)
}
$$
with

- $\delta = y - N_{\text{ch}}(\mu \cdot \gamma)$,
- $V = \epsilon^2 + N_{\text{ch}}(\mu \cdot \sigma^2)$,
- $v = \gamma + \dfrac{\delta}{V}\sigma^2$,
- $\Sigma_{\text{post}}$ as in §2.2 evaluated at $p_0=\mu$.

This completes the full derivation.
