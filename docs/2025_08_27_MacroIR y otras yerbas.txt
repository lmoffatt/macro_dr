
 -- Transcrito por zapia.com, tu IA personal
_Bueno, acá estoy en la reserva, voy a tratar de elaborar un plan para lo que tengo que hacer con Macro IR, específicamente la parte de código de programación. Bien, entonces..._
 -- Transcrito por zapia.com, tu IA personal
_Bueno, entonces acá, ¿cuál es el objetivo? Bueno, el objetivo, hay varios objetivos. Vamos a pensarlo por colaboraciones. Entonces, con la colaboración con Gustavo, que es la más firme, es continuar con este paper, con una continuación del paper que ya tenemos publicado, En el cual vamos a hacer nuevos modelos cinéticos que den cuenta de la información que pudimos obtener a partir de las dinámicas moleculares. Y específicamente eso. Y posiblemente de consideraciones también geométricas de esas dos cosas. Entonces la idea del paper es que en principio tenemos que tener consideraciones geométricas y moleculares y generar nuevos modelos. Un modelo que saldr de una directo es este que vos lo que ten es un acoplamiento en la tendencia a girar de las subunidades que est dada porque hay mucha superficie de contacto entre ellas Este Exactamente habr que ver si s s eso bueno eso por un lado este o sea que tengo acá dos cosas o sea una es hay un modelo que ya se empezaría a hacer inmediatamente después construir otros modelos más que tengan que ver con otras cosas bien Entonces, para poder correr ese modelo tenemos que tener macro IR funcionando. O sea, ya funcionaba antes. Aquí tengo una cuestión que podría hacerlo funcionar sin mucha duda. o sea, la única diferencia es eliminar la resolución temporal menor al límite de NIRSK. Eso tiene que darse. Entonces, bueno, eso casi, yo lo podría hasta correr con el programa como estaba directamente, pero yo no sé si hay alguna contra de hacer exactamente eso. Yo creo que en principio no habr ninguna contra no se tendr que pensarlo porque yo es lo que tengo Y ah es donde entra el tema de las emociones yo necesitaba recrear todo macroerre como sentirme que hac algo nuevo o sea hay una cosa ah que queríamos la parte mía emocional que va en contra de lo práctico no es cierto este pero bueno bueno En fin, entonces, si yo tengo el modelo este ya funcionando, lo que tendría que hacer es, al mismo tiempo, ir haciendo los tests que prueben de que eso funciona, ¿no es cierto? O sea, los tests de la evidencia, los tests de la likelihood, los tests del sampleo. y hoy en el baño había pensado otras herramientas más que tienen que ver con por ejemplo la relajación o sea empezar especialmente para lo que sea el sampleo, samplear sobre el máximo con un radio muy chiquito y después es como hacer una especie de expansión isotérmica, cambiar la temperatura bruscamente y ver c eso se expande para entender la velocidad del algoritmo la difusi del algoritmo Eso me parece que estar bueno se puede pensar eso como una especie de forma de medir algoritmos, medir características, caracterización de algoritmos, de montecarlo marco en eso me parece que sería un paper más y eso tengo que separarlo porque sería como como una idea más no saber cómo puedo caracterizar los algoritmos de montecarlo marco chain para para bueno encontrar los que sean óptimos no se encontrar este para uno que quieres Yo lo que quiero hacer es optimizar los algoritmos de Markov-Chain, conseguir los mejores posibles. Entonces, eso puede llevar mucho tiempo. Si tengo alguna medida más rápida de obtener de cómo esos algoritmos funcionan, es bueno para eso. Eso sería otro objetivo. Pero eso me parece que es un objetivo paralelo. Pero bueno. En fin. Bueno, entonces, lo que estoy llegando a la conclusión, bueno, esto lo voy a alargar, voy a seguir porque se me calambra el dedo._
 -- Transcrito por zapia.com, tu IA personal
_Ahí está. Ahí ya no se me acalambra más el dedo porque lo tengo totalmente fijado. Bueno, entonces ahora lo que tengo que hacer es tomar una decisión, que es lo que más me cuesta, ¿no es cierto? Yo creo que la decisión racional es rescatar el modelo que ya funcionaba y empezar a correr eso directamente con... Claro, ahí entra en conflicto, o sea, si yo ahora quiero hacer un modelo nuevo, lo tengo que hacer con esa vieja forma de trabajar, es decir, ese viejo código, pero bueno, en principio lo único que tendría que hacer es agregar un nuevo esquema, el esquema sería 16 posiblemente, 16 y 17 no sé cuáles serían habría que pensarlo un poco pero bueno si hacer un esquema 16 17 este y ya este largarlos a correr yo creo que los esquemas estos este ternarios este no los voy a considerar porque son un poco m dif de justificar Me meto en un lodazal de eso, no tiene sentido. Yo creo que el esquema este de la... el esquema que sería... tendría que definir cómo se llama, si 11, 12, 13, 15, 16 o 17, no sé. ese sería el que tengo que hacer y bueno, los otros el 6, el 7 y el 9 el 8 y el 9, bueno correrlos todos con los filtros de NIRSK, eso sería ya una cosa que tengo que hacer lo antes posible y ya lo puedo empezar a correr ya y ya mandarlos directamente al ¿cómo se llama? al cluster eso yo ya lo haría eso sería el punto 1 y después una vez que tenga el punto 1 el punto 2 es sí empezar a trabajar en que en este que este macro macro R y R sea un programa con como se dice o sea caracterizar a macro y R Es decir, hacer test de la Likely, hoot test de... A ver, ¿para qué es lo que tendría que caracterizar? Bueno, lo que tengo que caracterizar es en qué entornos de los parámetros que definen a los distintos algoritmos, que los algoritmos serían los de simulación, likelihood, sampleado y evidencia, en qué condiciones esos algoritmos funcionan y medianamente con qué eficiencia. Lo que tengo que hacer son gráficos de precisión y velocidad. Tengo que definir gráficos de precisión y velocidad para todos esos algoritmos y variables. Y lo puedo hacer en principio con un modelo minimalista. Empiezo con un modelo minimalista y quiz hago todo lo que pueda hacer de un modelo maximalista tambi En principio lo trabaj todo con uno o dos modelos minimalistas dos modelos en el sentido de poder comparar alternativas o sea hay una de las cosas que tengo que definir es bueno cuáles serían los dos modelos minimalistas para comparar podría ser por ejemplo el 1 y el 2 es es decir, la presencia o no del flip state, puede ser, o por ejemplo que sean dos o tres sitios de unión, o uno o dos abiertos, por ejemplo, serían como casos fáciles de trabajar con pocos datos, Y ahí entonces hago un estudio en profundidad de esas cosas, de cómo los distintos parámetros afectan la resolución y la velocidad de la obtención de parámetros cinéticos a partir de... ¿Cómo se dice? Corrientes macroscópicas._
 -- Transcrito por zapia.com, tu IA personal
_Bueno, entonces, a ver, quedó definido que lo que voy a hacer es reconstruir el código que ya funcionaba y ponerlo a correr inmediatamente con un esquema donde interactúan entre, como se dice, la rotación, hay interacción aerosérica en la rotación. Eso es el punto número 1. Así lo largo como está. Bien, punto número 2 es también me pongo a optimizar distintos parámetros de los distintos algoritmos. O sea, primero, no es solamente optimizar, el primero es verificación y optimización de los distintos componentes de macro IR. Los distintos componentes son Simulation, Lightlyhood, Sampling y Evidence. Esos son los cuatro. Y cada uno tiene sus algoritmos, o sea, sus definiciones de algoritmos. Esos algoritmos tienen par y entonces yo lo que tengo que ver es cu son las regiones donde esos par garantizan o se encuentra que se cumple con los test de cada uno de ellos Yo tengo que definir primero, entonces la primera tarea es definir para cada uno de estos cuatro algoritmos, cuáles son las condiciones de corrección, cuáles son la precisión y la velocidad. Esas son las, y claro, el número de datos. O sea, para poder definir, como se dice, velocidad, yo tengo que definir output. O sea, cuál es mi output. Entonces, cuál es mi output, que ese output sea correcto y cuál es el error de ese output. O sea, serían las cosas que tengo que definir para cada uno de los cuatro algoritmos. yo creo que si yo defino eso es una cuestión bastante teórica en sí mismo es una gran avance no es decir de definirlo as eso est muy bien es decir eso eso ese es el otro punto entonces en primer punto es hacer que de vuelta ande este macro r como estaba igual los igual que en el paper pero con el l de nist pero con un modelo nuevo que es este que estamos planteando eso y después el otro punto es definir los cuatro algoritmos de macro de r importantes en realidad serían seis más serían dijimos simulation like lijo sampling y después tengo derivat y derivat y de que y de la iclijo de este de la iclijo fundamentalmente igual no yo tengo ahora después el otros parámetros derivados que es bueno el sampling y obtenemos con combinaciones de por ejemplo simulation y likelihood puedo obtener y el y el derivativo es decir el score no sería score quizás uno podría plantearlo como score y fin y ahí yo tengo que definir exactamente d est el algoritmo por el algoritmo de derivadas es este por un lado tengo el algoritmo score y el algoritmo que se llamada fin pero ellos los construy a partir de un meta algoritmo que ser el de derivate pero bueno pero derivate y ben realidad digamos si yo los tomo como algoritmo también tiene que tomar aquello al que la linea l y resolución de ecuaciones diferenciales, un montón de cosas que no vienen al caso, que son importantes, pero bueno, exceden. Entonces yo creo que tendría que ser, entonces serían así. A ver, los algoritmos son, bueno, dijimos, simulation, likelihood, score, fissioned performance matrix, sampling y evidence. Claro, sampling y evidence son, digamos, de alguna manera, algoritmos compuestos pero bueno, que es sampling evidence sobre likelihood pero que a veces likelihood a su vez tiene como se dice un likelihood más derivativo más Fisher Information Matrix bueno, eso sería_
 -- Transcrito por zapia.com, tu IA personal
_Bueno, entonces me queda el elefantón de RUM, que es todo este tiempo que estuve pensando en la command line interface, con semánticamente meaningful. Entonces la pregunta es, ¿es necesario tanto dolor para llegar a eso? entonces cómo voy a hacer para para digamos implementar mi nuevo macro DR mejor organizado y todo eso y que no sea tanto trabajo porque hasta ahora me está como llevando mucho mucho trabajo esa parte es la más difícil y claro, muchas veces me suena como incompleta ¿cómo yo puedo hacer para reorganizar macro DR para que sea un poco más ordenado y no tan complejo y difícil de pensar y manejar? o sea, eso es un problema difícil y que lleva tiempo y que quiz lo que tenga que hacer es renunciar a hacerlo antes del fin de a sea quiz est lo m razonable sea dejar ese proyecto de lado o sea quizás y o sea con pequeñas cosas como reorganizar los capaces ya se empiezo a empiezo a transar la transa es infinidad donde pongo el límite no yo creo que lo mejor es lo siguiente, es que tengo dos versiones, la versión vieja que voy a correr en el cluster con ella inmediatamente con este con el nuevo modelo y después la versión nueva que sigue su curso con lo que tengo hasta ahora, que lo voy integrando bien digamos, tratando de hacer una versión renovada y una versión más presentable a la comunidad, que sería como la versión 1.0, no sé cómo llamarla, con todas prácticas más mejores y qué sé yo. Y esa versión, la pregunta es, ¿esa versión la voy a usar para hacer los test de optimización o no? Y yo creo que s tendr que usar esa versi para eso S claramente Bien y en caso que alg alg test me falle as medio bestialmente bueno entonces ah s la versión vieja se deja de lado y se vuelve a hacer igual volvería a hacerlo igual con la versión nueva o sea que es lo mismo más o menos está bien entonces operativamente tengo dos versiones una versión vieja con el nuevo modelo y la versión nueva que sigue evolucionando y la versión nueva que es cuáles son los objetivos bueno el objetivo es este hacerlos todos estos esta caracterización de macro r con todas las dimensiones que ya habíamos dicho de bueno que necesito ahora las palabras pero que tiene que ver con que el código se verifique que corre correctamente en qué condiciones tengo error en determinado tipo de error nivel de error y qué condiciones determinada velocidad para cada uno de los siete comandos que son simulaci likelihood score FIM sampling y evidence Y de todos ellos tengo que estudiar medianamente cu son las condiciones en que el c est verificado y qué condiciones es más rápido o efectivo o lo que sea. bien y claro, y la otra es idealization sería un otro algoritmo creo yo también que habría que incluir, bueno eso ya entra con el tema de cumulative evidence yo creo que idealization no lo voy a no lo voy a manejar en esta sería otro otro paper que es el de cumulative evidence que sería el siguiente paper que sigue a este de characterization de macro DR. Entonces tengo characterization de macro DR y el segundo paper con Gustavo, serían las dos cosas que van en paralelo. Luego voy a ver cómo abro el nuevo línea con Cecilia, a ver qué es lo que puedo usar. que esa es la línea de Cecilia supongo que esa va a tardar un poco más y posiblemente se monte en la caracterización de macro DR primero bien_
 -- Transcrito por zapia.com, tu IA personal
_Bueno, entonces vamos a cerrar ya todo esto. Punto 1. Hacer correr lo antes posible el modelo 16 o como llamarlo. Punto 2. Caracterizar macro DR. Y bueno, ya está. punto 3 sería cumulative evidence, yo creo y punto 4 es correrlo en clusters, en Ecosamae es en GPUs, macro GPU, macro R GPU Ah, punto 5 Ah, ahí está, ese es más importante Antes que Macro R GPU Es Global Macro R Que sería poder optimizar al mismo tiempo Otro tipo de datos Que son datos de equilibrio Es decir, yo voy a manejar experimentos más complicados que tengan información cinética de distintas fuentes. Pero eso lo voy a hacer posiblemente SSA y también de canales únicos. Eso es el todo tema. Yo creo que, digamos, ositos y canales únicos serían las dos cosas que quedarían. O sea, que sería lo que probablemente termine trabajando con Cecilia Boussat. Hay mutantes Mutantes desde el punto de vista Teórico Hay muchas cosas_
 -- Transcrito por zapia.com, tu IA personal
_Bueno, vamos entonces con un resumen de los... O sea, me quedan entonces dos papers a trabajar de inmediato. Este, que son... Bueno, acá continuamos. Estaba diciendo que ya tenemos dos papers definidos. O sea, digamos, el objetivo de dos papers. Él, a continuación, con Gustavo Zuno. Uno, ya me pongo a trabajar en ese modelo, y la caracterización de macro DR es el otro. Bien, una vez que termino esos, ¿cuál es el más importante que más cosas nos puede dar? Bueno, tengo varias cosas para analizar. O sea si lo planteamos desde el punto de vista de objetivos bueno ser poder digamos entender experimentos que involucren mutaciones Por un lado experimentos que sean en ositos es decir que sean m lentos por otro lado y experimentos que sean de canal sería la tercera cosa, que es hacer un poco lo más difícil y complejo, y lo de canal único yo creo que eso tengo que hacer una colaboración con más gente para llegar a un punto, probablemente con Lorin y algún otro más, posiblemente con este, claro, tenga que volver con la gente de Cojum, por lo menos de trabajarlo. de trabajarlo, eso yo creo que es importante, creo que de todos los problemas, o sea, volver a canales únicos para definir cuál es la likelihood con el tema de la idealización, eso ser un golazo o sea el tema con canales claro que hay el programa en realidad es conforme al modo de no sea es conforme al model m claro conforme al model con evidencia y aplicado a a ositos corrientes con ositos con mutaciones también y luego con formation model y evidencia aplicado a canales únicos a corrientes con un canal único ese creo que es el más bueno, no es tan difícil porque en realidad la pregunta es cómo se puede adaptar los caros cómo se pueden adaptar las idealizaciones el tema con la idealización es que vos ten una idealizaci pero desech otras idealizaciones posibles tom la tom la que tiene m la de clic pero no no tomas las otras y este y ah est un poco claro y vos tenés un error en los parámetros pero no tenés un error en los la de clic o sea con eso lo que habría que trabajar es como vos de alguna manera estás estás conseguí hablando la la iclijudo sam eso es todo un tema habría que plantearlo y ver cómo se puede solucionar pero el tema de canales únicos es un tema teórico muy importante muy interesante que es fundamental abordar y liquidar de una vez sí_
 -- Transcrito por zapia.com, tu IA personal
_entonces cómo plantear el tema de la likelihood de modelos idealizados de canales iónicos me parece que yo me acuerdo que Cojum decía que no se podía hablar de likelihood, yo decía ¿por qué dice que no? y es por eso, quizás lo podría hacer con David directamente no estaría mal, habría que ver un poco cómo lo plantean ellos Ese es un problema teórico que podría ya mismo ponerme a trabajar, porque la idea no es solucionar el problema computacionalmente, sino por ahí teóricamente involucrar a otros investigadores importantes en eso, antes de plantear una resolución algorítmica quizás. Y el otro problema sí es más fácil, el de conformational model más conformational model con corrientes, con los hitos. Eso sí. Bueno, también vos podés, qué sé yo, con canales únicos ver cosas como la conductancia o ver un poco... Claro, pasa que igual, el tema es que con la duración de las aperturas y clausuras, el tema con las idealizaciones es complejo, hay que solucionarlo de alguna manera, no es tan fácil._
 -- Transcrito por zapia.com, tu IA personal
_Bueno, los otros dos problemas que habría que plantear es, bueno, una es el efecto del potencial de membrana y otra es la desinsitización. Sí, y bueno, esos son otros problemas que habría que estudiar dedicadamente._
 -- Transcrito por zapia.com, tu IA personal
_bueno una de las preguntas es cómo me organizo no si yo tengo distintas carpetas para cada uno de los proyectos distintas como se dice esto ramas branches para cada uno de sus proyectos ramas de macro de r yo creo que eso estaría bien las ramas creo que sí tengo que abrir las distintas carpetas para cada uno de los papers, proyectos. El que me queda totalmente atrás es el de la programación con tipos, typas, proposition, en programing, eso es como que me queda un poco alejado. Y eso más que nada porque no tengo una comunidad alca a la cual rendirle cuentas. Entonces, digamos, en este momento no tiene sentido apuntar a eso, porque no tengo esa comunidad pero pero bueno quizás el año que viene si me pueda poner con eso_
 -- Transcrito por zapia.com, tu IA personal
