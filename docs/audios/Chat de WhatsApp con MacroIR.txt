27/8/25, 10:15 - Los mensajes y las llamadas están cifrados de extremo a extremo. Solo las personas en este chat pueden leerlos, escucharlos o compartirlos. Obtén más información.
27/8/25, 10:15 - Creaste este grupo
27/8/25, 10:15 - Se añadió a Andrea Bragas
27/8/25, 10:15 - Eliminaste a Andrea Bragas.
27/8/25, 10:18 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 10:18 - Luciano Moffatt: _Bueno, acá estoy caminando de nuevo por la 9 de julio, hay bastante tránsito, espero que esto se pueda escuchar bien. La verdad que no sé bien qué quiero decir, la idea es un poco planificando mis próximos trabajos, o sea, qué es lo que puedo publicar en los próximos meses. En principio tengo dos colaboraciones ya yendo, una es con Gustavo Pierdominici y la otra es con Cecilia Bousat. Con Gustavo, Gustavo le planteé hacer un modelo donde haya interacción explícita entre la rotación de subunidades, que la rotación de una subunidad depende del estado rotacional de las subunidades contiguas, es decir, cada una de las subunidades rotadas aporta un equilibrio hacia la rotación de esa subunidad. Eso es una f continuaci de nuestro paper habr que ver si hay algo m que pueda surgir de las simulaciones de Gustavo que puedan aportar a eso, ¿no? Alguna otra sutileza más. O algo que represente claramente los resultados que él sacó. Eso es una cosa. Después con Cecilia, bueno, estoy abierto a ver qué es lo que ella le parece que pueda yo aportar con mis tecnologías para los intereses de ella y muy abierto. Pero en principio lo que podemos buscar es ver si puede aplicarse modelos alostéricos a receptores CIS-LU. Bien, esas serían las dos cosas que tengo, digamos. Ahora, para lo primero, desde el punto de vista del algoritmo, lo que tengo que tener es lo mismo que tenía antes, nada diferente, solamente que funcionando y con cierta fe en que los resultados sean buenos._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 10:18 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 10:18 - Luciano Moffatt: _Bueno, entonces en principio lo que tengo que tener funcionando es el algoritmo como lo tenía antes del PP pero con un poco más de sentirme cómodo con ese algoritmo. En particular que los tests funcionen a partir de la interfaz de comandos. O sea que a partir de la interfaz de comandos yo pueda hacer test que indiquen que el algoritmo funciona. Entonces la pregunta es ¿cuáles son esos test? Bueno, el primer test es verificar que la simulación de corrientes hechas por el algoritmo corresponde. Y después la otra es un test respecto de la validez de la función de likelihood. ese test de la validez de la función de likelihood lo hago a partir de la generación de muchas muestras usando la simulación de un esquema cinético y la idea es que la esperanza de la covarianza del score tiene que ser igual a la esperanza de la informaci de matriz de Fischer Eso, digamos, y lo más importante es que evaluado eso en los valores de parámetros que corresponden a la distribución simulada, la esperanza del score tiene que ser cero y tengo que poder hacer un cero con un test basado en la información de Fisher, es decir que sea, no va a ser exactamente cero pero digamos cero más menos digamos, el entorno de error de la función de Fisher. Y además el tema este de que la esperanza, la covarianza, perdón, la covarianza del score tiene que ser igual a la esperanza de la matriz de información de Fisher._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 10:18 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 10:18 - Luciano Moffatt: _Bueno, finalmente me queda el tema, con eso verificaría que la función del ARK-HUD funciona. Luego me queda el tema de cómo yo verifico que la implementación de la estimación de la evidencia es correcta. Bueno, ahí cómo hacemos. ¿cómo hacemos ahí? bueno, ahí habría que en principio se podría hacer una función de una matriz de confusión respecto de de poder simular experimentos con un modelo y después fitearlos con varios modelos incluyendo el modelo en el cual fue simulado y que pueda distinguir eso de otros modelos. O sea, esa sería la prueba de fuego de este algoritmo. Bueno, eso lo podemos hacer, digamos, para condiciones que sean más simples. O sea, modelos que converjan rápido o que sean relativamente simples. los podemos probar por lo menos con modelos lineales. Los otros podemos hacer con modelos bastante complejos, sin mucha historia._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 10:18 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 10:18 - Luciano Moffatt: _Entonces esto sería un tercer paper, una colaboración conmigo mismo, o sea con nadie más, que lo que haga básicamente sea explorar todas las cosas, o sea publicar todos los controles, todas las pruebas que yo hago para verificar que el algoritmo es el correcto. Eso sería un paper largo y tedioso que publicaré en algún lugar. En principio podría ser en BioArchives y después lo mando a algún otro lado. Pero que sería, digamos, como una verificación bastante formal de cómo funciona el algoritmo y viendo los distintos detalles. O sea, ahí podría ponerme un poquito denso, o sea, un poquito más sistemático y tratar de ir explorando las distintas cosas, o sea, tratando de... no explorar las distintas cosas, es decir, a ver cómo depende, por ejemplo, qué sé yo, el error de los datos, o sea, por ejemplo, a ver, a medida que yo aumento el número de puntos, cómo mejora la resolución del algoritmo en cuanto a los parámetros cinéticos en qué punto, por ejemplo, si yo tengo muy pocos datos no voy a poder distinguir entre modelos entonces puede ser que un modelo más simple que incorrecto fitea los datos y otro correcto no eso podr ocurrir tranquilamente o sea tiene que haber circunstancias donde eso ocurra y eso ser interesante explorarlo o sea eso ser como otro tipo de preguntas que se podrían hacer, claramente o también, por ejemplo, el otro tema muy importante es la likelihood distintos algoritmos de likelihood que tengo, cuáles son las condiciones en las cuales las aproximaciones son buenas eso también es un tema muy importante, es decir yo lo que tendría que ver es los distintos, las tres funciones importantes de macro R que serían simulation, likelihood y evidence ver digamos en qué condiciones funcionan, es decir claro, para eso es ver los tests o sea, tener un valorado un test y ver, bueno, ese test ver en qué circunstancias se cumplen, en qué circunstancias no se cumplen y también la velocidad, ¿no? ¿De qué depende la velocidad del test? Como para tener una idea del ámbito donde esto funciona. O sea, eso me parece que sería muy piola hacerlo al paralelo al trabajo con Gustavo. Es decir, mi plan tiene que ser tener algo más o menos que funcione rápido. O sea, en realidad podría empezar con esta segunda parte, es decir, tener los test, una vez que tengo los test de simulación, test de slide y test de evidencia, ya puedo empezar a hacer experimentos, entonces empiezo a hacer experimentos con modelos simples, y al mismo tiempo que estoy haciendo esos experimentos, voy pensando los modelos que tengo que fitear para Gustavo. Y mientras tanto hablo con Cecilia para ver, bueno, también lo mismo, o no sé a qué modelos ella necesita fitear o queda tostía._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 10:18 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 10:18 - Luciano Moffatt: _Entonces volviendo a esto del tercer paper, que sería el segundo paper en realidad, que sería, digamos, una presentación formal de macro R, o sea, ver macro R y macro IR, en qué condiciones de número de datos, etc., de puntos, funciona, o sea, cómo vos podés diferenciar entre modelos de acuerdo a la cantidad de datos que tenés, etc., tratando también de ver el tema de la resolución temporal. Y quizás ahí también podemos meter con cumulative evidence, que eso sería un tercer paper, porque me parece que eso está separado. O sea, yo lo de cumulative evidence lo dejaría como para un tercer paper._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 10:18 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 10:18 - Luciano Moffatt: _Bueno, a ver, ¿cuáles serían los comandos que tiene que tener mi sistema? Bueno, los comandos que tenía eran, primero que nada, evidencia. La evidencia está basada en la evidencia de un modelo para una distribución de parámetros, para unos datos y con algún algoritmo de evidencia y algoritmo de likelihood. o sea tenemos todos esos condimentos bien, después tenemos la función de likelihood que tiene todas las evidencias menos el algoritmo de evidencia o sea tiene un modelo, tiene los priors del modelo, los datos y el algoritmo de likelihood después tenemos la simulación que tiene el algoritmo de simulaci el modelo y los datos y un par no tiene par lo que no tiene es la se llama Los priors de par Despu podr inventar una que sea un tipo sampling No evidencia, sino sampling, que sería como el posterior, ¿no? Algo así como el posterior, habría que ver cuál es el nombre, que tendría que tener todos los mismos que la evidencia, menos el algoritmo de evidencia, tendría que tener un algoritmo de MSM, de Monte Carlo Markov Chain. Y después podríamos tener un algoritmo de idealización de datos, o sea que transformes un experimento en una submuestra de experimentos. Después tendríamos también una de construcción de modelos y una de construcción de distribuciones a priori. Y después una definición de experimentos. Esas son todas las funciones que tengo que definir._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 10:18 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 10:18 - Luciano Moffatt: _Entonces, con eso todo definido, empiezo a pensar qué cosas me quedarían a mí por definir. Bueno, digamos, lo más controversial, entre comillas, serían, bueno, los datos y los modelos, o sea, cómo definir los datos, o sea, cómo definir el experimento. porque claro esto tendría que estar basado en los experimentos originales es decir los records de de action binary files o algo así y a partir de eso es armar los experimentos eso sería, pero claro eso tendría que ser una versión 2.0 una primera versión tomaría los datos tal cual los tengo ahora bien eso respecto de los datos, respecto del modelo también lo mismo, o sea, tendría que tener primero los modelos como los tengo ahora, o poder definir los modelos a partir de la, se llama la l de comandos o sea tener un DSL es decir un lenguaje espec de dominio espec para definir modelos, también un lenguaje de dominio específico para definir experimentos y después tendríamos las distribuciones a priori de los parámetros y los algoritmos diferentes que tenemos que definir, el algoritmo de simulación, el algoritmo de likelihood, el algoritmo de evidencia y el algoritmo de sampleado. Y luego tendríamos, la otra cosa es el algoritmo de derivada, o sea, de cómo tomar la derivada, que tenemos que tomar la derivada de la likelihood y para poder samplear el score y después lo que sería también la ficha Information Matrix, que eso lo necesitamos todo para poder hacer los tests, si no, no podemos hacer los tests de que funcione la likelihood._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 10:18 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 10:18 - Luciano Moffatt: _Bueno, y acá me quedaría por definir si hay alguna manera de testear los algoritmos de likelihood que no involucren al score. Es decir, ¿puedo testear la likelihood sin score? Y la respuesta es que en principio no. O sea, podría también calcular la likelihood esperada como otra variable más, que tendría que ser otro comando más, la likelihood esperada. ¿Qué más puede ser? Y claro, ahí no sé, bueno, tendría la... Si tengo derivadas, puedo tener la derivada de la likelihood esperada, que no sé qué carajo significa. pero digamos, el problema con la ley cricudesperada es que eso ya lo vi que no te diferencia digamos, una institución que anda de una que no anda pero bueno no s no no no se me ocurre pensar alguna forma de d testeo de la likelihood está yo me acuerdo que bueno pensé mucho en el tema de distribución que vos este de probabilidad y sampleado no este pero que si vos tenés una likelihood y un sampleado vos podrías ver que una corresponde a la otra bueno eso todo eso tiene que ver con el tema que yo ya pensé bastante que era el tema de la likelihood y la la probabilidad y el sampling como como inversas y eso bueno no llegué a ningún puerto con eso, estuvimos mucho tiempo pensándolo y no llegamos a nada_
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 10:18 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 10:18 - Luciano Moffatt: _Bueno, entonces quedaría definido el plan. Una vez que tenga toda esta serie de comandos, que ya hablé más o menos todos definidos, entonces me quedarían experimentos, entre comillas, para hacer que corresponderían a algún tipo de paper. que eso tendríamos que definirlo después entonces el paper sería una serie de experimentos para hacer con estos comandos y una vez que se hacen los experimentos se hacen las figuras y se escribe el paper ahora vamos a tratar de adelantarnos un poquito y pensar cuáles serían esos experimentos Bueno, fácil es en principio pensar especialmente en lo que definen a los distintos parámetros que definen los algoritmos, Sí, los algoritmos, el algoritmo de la EIC, el algoritmo de la evidencia, el de sampleo, el de los dos sampleos, el de posteriori y el de sampleo de simulación, tendría que un poco trabajar en qué rangos de esos algoritmos las cosas funcionan. Y tambi el tema de como un dato de cada comando es qu precisi se tiene o sea tendr que haber un concepto de error de error en cuanto a apartamiento del valor óptimo o ideal o verdadero, y el de tiempo, ¿no? O sea, porque la idea es con esas dos cosas, la combinación de tiempo, de cálculo y precisión, es, digamos, se puede optimizar los cálculos para que, dado que uno tiene la cantidad determinada de tiempo de cómputo, qué precisión se puede llegar. Que eso sería un poco, uno podría plantearlo como uno de los objetivos del trabajo, es como tiempo y precisión en el cálculo y datos. O sea, vos tendrías cuánto tiempo de cálculo, qué precisión querés obtener en cuanto a los parámetros cinéticos o de poder diferenciar modelos, o sea, cuánta resolución, cuánto tiempo de cálculo y cuántos datos mismos necesitas para alcanzar todo eso. y por ahí posiblemente que simplificación de los datos también, pero ahí ya en ese último tema entraríamos en el cumulative evidence que eso sería para mí un nuevo otro paper_
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 10:51 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 10:52 - Luciano Moffatt: _Bueno, acá estoy en la reserva, voy a tratar de elaborar un plan para lo que tengo que hacer con Macro IR, específicamente la parte de código de programación. Bien, entonces..._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 10:57 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 10:58 - Luciano Moffatt: _Bueno, entonces acá, ¿cuál es el objetivo? Bueno, el objetivo, hay varios objetivos. Vamos a pensarlo por colaboraciones. Entonces, con la colaboración con Gustavo, que es la más firme, es continuar con este paper, con una continuación del paper que ya tenemos publicado, En el cual vamos a hacer nuevos modelos cinéticos que den cuenta de la información que pudimos obtener a partir de las dinámicas moleculares. Y específicamente eso. Y posiblemente de consideraciones también geométricas de esas dos cosas. Entonces la idea del paper es que en principio tenemos que tener consideraciones geométricas y moleculares y generar nuevos modelos. Un modelo que saldr de una directo es este que vos lo que ten es un acoplamiento en la tendencia a girar de las subunidades que est dada porque hay mucha superficie de contacto entre ellas Este Exactamente habr que ver si s s eso bueno eso por un lado este o sea que tengo acá dos cosas o sea una es hay un modelo que ya se empezaría a hacer inmediatamente después construir otros modelos más que tengan que ver con otras cosas bien Entonces, para poder correr ese modelo tenemos que tener macro IR funcionando. O sea, ya funcionaba antes. Aquí tengo una cuestión que podría hacerlo funcionar sin mucha duda. o sea, la única diferencia es eliminar la resolución temporal menor al límite de NIRSK. Eso tiene que darse. Entonces, bueno, eso casi, yo lo podría hasta correr con el programa como estaba directamente, pero yo no sé si hay alguna contra de hacer exactamente eso. Yo creo que en principio no habr ninguna contra no se tendr que pensarlo porque yo es lo que tengo Y ah es donde entra el tema de las emociones yo necesitaba recrear todo macroerre como sentirme que hac algo nuevo o sea hay una cosa ah que queríamos la parte mía emocional que va en contra de lo práctico no es cierto este pero bueno bueno En fin, entonces, si yo tengo el modelo este ya funcionando, lo que tendría que hacer es, al mismo tiempo, ir haciendo los tests que prueben de que eso funciona, ¿no es cierto? O sea, los tests de la evidencia, los tests de la likelihood, los tests del sampleo. y hoy en el baño había pensado otras herramientas más que tienen que ver con por ejemplo la relajación o sea empezar especialmente para lo que sea el sampleo, samplear sobre el máximo con un radio muy chiquito y después es como hacer una especie de expansión isotérmica, cambiar la temperatura bruscamente y ver c eso se expande para entender la velocidad del algoritmo la difusi del algoritmo Eso me parece que estar bueno se puede pensar eso como una especie de forma de medir algoritmos, medir características, caracterización de algoritmos, de montecarlo marco en eso me parece que sería un paper más y eso tengo que separarlo porque sería como como una idea más no saber cómo puedo caracterizar los algoritmos de montecarlo marco chain para para bueno encontrar los que sean óptimos no se encontrar este para uno que quieres Yo lo que quiero hacer es optimizar los algoritmos de Markov-Chain, conseguir los mejores posibles. Entonces, eso puede llevar mucho tiempo. Si tengo alguna medida más rápida de obtener de cómo esos algoritmos funcionan, es bueno para eso. Eso sería otro objetivo. Pero eso me parece que es un objetivo paralelo. Pero bueno. En fin. Bueno, entonces, lo que estoy llegando a la conclusión, bueno, esto lo voy a alargar, voy a seguir porque se me calambra el dedo._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 11:03 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 11:04 - Luciano Moffatt: _Ahí está. Ahí ya no se me acalambra más el dedo porque lo tengo totalmente fijado. Bueno, entonces ahora lo que tengo que hacer es tomar una decisión, que es lo que más me cuesta, ¿no es cierto? Yo creo que la decisión racional es rescatar el modelo que ya funcionaba y empezar a correr eso directamente con... Claro, ahí entra en conflicto, o sea, si yo ahora quiero hacer un modelo nuevo, lo tengo que hacer con esa vieja forma de trabajar, es decir, ese viejo código, pero bueno, en principio lo único que tendría que hacer es agregar un nuevo esquema, el esquema sería 16 posiblemente, 16 y 17 no sé cuáles serían habría que pensarlo un poco pero bueno si hacer un esquema 16 17 este y ya este largarlos a correr yo creo que los esquemas estos este ternarios este no los voy a considerar porque son un poco m dif de justificar Me meto en un lodazal de eso, no tiene sentido. Yo creo que el esquema este de la... el esquema que sería... tendría que definir cómo se llama, si 11, 12, 13, 15, 16 o 17, no sé. ese sería el que tengo que hacer y bueno, los otros el 6, el 7 y el 9 el 8 y el 9, bueno correrlos todos con los filtros de NIRSK, eso sería ya una cosa que tengo que hacer lo antes posible y ya lo puedo empezar a correr ya y ya mandarlos directamente al ¿cómo se llama? al cluster eso yo ya lo haría eso sería el punto 1 y después una vez que tenga el punto 1 el punto 2 es sí empezar a trabajar en que en este que este macro macro R y R sea un programa con como se dice o sea caracterizar a macro y R Es decir, hacer test de la Likely, hoot test de... A ver, ¿para qué es lo que tendría que caracterizar? Bueno, lo que tengo que caracterizar es en qué entornos de los parámetros que definen a los distintos algoritmos, que los algoritmos serían los de simulación, likelihood, sampleado y evidencia, en qué condiciones esos algoritmos funcionan y medianamente con qué eficiencia. Lo que tengo que hacer son gráficos de precisión y velocidad. Tengo que definir gráficos de precisión y velocidad para todos esos algoritmos y variables. Y lo puedo hacer en principio con un modelo minimalista. Empiezo con un modelo minimalista y quiz hago todo lo que pueda hacer de un modelo maximalista tambi En principio lo trabaj todo con uno o dos modelos minimalistas dos modelos en el sentido de poder comparar alternativas o sea hay una de las cosas que tengo que definir es bueno cuáles serían los dos modelos minimalistas para comparar podría ser por ejemplo el 1 y el 2 es es decir, la presencia o no del flip state, puede ser, o por ejemplo que sean dos o tres sitios de unión, o uno o dos abiertos, por ejemplo, serían como casos fáciles de trabajar con pocos datos, Y ahí entonces hago un estudio en profundidad de esas cosas, de cómo los distintos parámetros afectan la resolución y la velocidad de la obtención de parámetros cinéticos a partir de... ¿Cómo se dice? Corrientes macroscópicas._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 11:09 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 11:09 - Luciano Moffatt: _Bueno, entonces, a ver, quedó definido que lo que voy a hacer es reconstruir el código que ya funcionaba y ponerlo a correr inmediatamente con un esquema donde interactúan entre, como se dice, la rotación, hay interacción aerosérica en la rotación. Eso es el punto número 1. Así lo largo como está. Bien, punto número 2 es también me pongo a optimizar distintos parámetros de los distintos algoritmos. O sea, primero, no es solamente optimizar, el primero es verificación y optimización de los distintos componentes de macro IR. Los distintos componentes son Simulation, Lightlyhood, Sampling y Evidence. Esos son los cuatro. Y cada uno tiene sus algoritmos, o sea, sus definiciones de algoritmos. Esos algoritmos tienen par y entonces yo lo que tengo que ver es cu son las regiones donde esos par garantizan o se encuentra que se cumple con los test de cada uno de ellos Yo tengo que definir primero, entonces la primera tarea es definir para cada uno de estos cuatro algoritmos, cuáles son las condiciones de corrección, cuáles son la precisión y la velocidad. Esas son las, y claro, el número de datos. O sea, para poder definir, como se dice, velocidad, yo tengo que definir output. O sea, cuál es mi output. Entonces, cuál es mi output, que ese output sea correcto y cuál es el error de ese output. O sea, serían las cosas que tengo que definir para cada uno de los cuatro algoritmos. yo creo que si yo defino eso es una cuestión bastante teórica en sí mismo es una gran avance no es decir de definirlo as eso est muy bien es decir eso eso ese es el otro punto entonces en primer punto es hacer que de vuelta ande este macro r como estaba igual los igual que en el paper pero con el l de nist pero con un modelo nuevo que es este que estamos planteando eso y después el otro punto es definir los cuatro algoritmos de macro de r importantes en realidad serían seis más serían dijimos simulation like lijo sampling y después tengo derivat y derivat y de que y de la iclijo de este de la iclijo fundamentalmente igual no yo tengo ahora después el otros parámetros derivados que es bueno el sampling y obtenemos con combinaciones de por ejemplo simulation y likelihood puedo obtener y el y el derivativo es decir el score no sería score quizás uno podría plantearlo como score y fin y ahí yo tengo que definir exactamente d est el algoritmo por el algoritmo de derivadas es este por un lado tengo el algoritmo score y el algoritmo que se llamada fin pero ellos los construy a partir de un meta algoritmo que ser el de derivate pero bueno pero derivate y ben realidad digamos si yo los tomo como algoritmo también tiene que tomar aquello al que la linea l y resolución de ecuaciones diferenciales, un montón de cosas que no vienen al caso, que son importantes, pero bueno, exceden. Entonces yo creo que tendría que ser, entonces serían así. A ver, los algoritmos son, bueno, dijimos, simulation, likelihood, score, fissioned performance matrix, sampling y evidence. Claro, sampling y evidence son, digamos, de alguna manera, algoritmos compuestos pero bueno, que es sampling evidence sobre likelihood pero que a veces likelihood a su vez tiene como se dice un likelihood más derivativo más Fisher Information Matrix bueno, eso sería_
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 11:23 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 11:24 - Luciano Moffatt: _Bueno, entonces me queda el elefantón de RUM, que es todo este tiempo que estuve pensando en la command line interface, con semánticamente meaningful. Entonces la pregunta es, ¿es necesario tanto dolor para llegar a eso? entonces cómo voy a hacer para para digamos implementar mi nuevo macro DR mejor organizado y todo eso y que no sea tanto trabajo porque hasta ahora me está como llevando mucho mucho trabajo esa parte es la más difícil y claro, muchas veces me suena como incompleta ¿cómo yo puedo hacer para reorganizar macro DR para que sea un poco más ordenado y no tan complejo y difícil de pensar y manejar? o sea, eso es un problema difícil y que lleva tiempo y que quiz lo que tenga que hacer es renunciar a hacerlo antes del fin de a sea quiz est lo m razonable sea dejar ese proyecto de lado o sea quizás y o sea con pequeñas cosas como reorganizar los capaces ya se empiezo a empiezo a transar la transa es infinidad donde pongo el límite no yo creo que lo mejor es lo siguiente, es que tengo dos versiones, la versión vieja que voy a correr en el cluster con ella inmediatamente con este con el nuevo modelo y después la versión nueva que sigue su curso con lo que tengo hasta ahora, que lo voy integrando bien digamos, tratando de hacer una versión renovada y una versión más presentable a la comunidad, que sería como la versión 1.0, no sé cómo llamarla, con todas prácticas más mejores y qué sé yo. Y esa versión, la pregunta es, ¿esa versión la voy a usar para hacer los test de optimización o no? Y yo creo que s tendr que usar esa versi para eso S claramente Bien y en caso que alg alg test me falle as medio bestialmente bueno entonces ah s la versión vieja se deja de lado y se vuelve a hacer igual volvería a hacerlo igual con la versión nueva o sea que es lo mismo más o menos está bien entonces operativamente tengo dos versiones una versión vieja con el nuevo modelo y la versión nueva que sigue evolucionando y la versión nueva que es cuáles son los objetivos bueno el objetivo es este hacerlos todos estos esta caracterización de macro r con todas las dimensiones que ya habíamos dicho de bueno que necesito ahora las palabras pero que tiene que ver con que el código se verifique que corre correctamente en qué condiciones tengo error en determinado tipo de error nivel de error y qué condiciones determinada velocidad para cada uno de los siete comandos que son simulaci likelihood score FIM sampling y evidence Y de todos ellos tengo que estudiar medianamente cu son las condiciones en que el c est verificado y qué condiciones es más rápido o efectivo o lo que sea. bien y claro, y la otra es idealization sería un otro algoritmo creo yo también que habría que incluir, bueno eso ya entra con el tema de cumulative evidence yo creo que idealization no lo voy a no lo voy a manejar en esta sería otro otro paper que es el de cumulative evidence que sería el siguiente paper que sigue a este de characterization de macro DR. Entonces tengo characterization de macro DR y el segundo paper con Gustavo, serían las dos cosas que van en paralelo. Luego voy a ver cómo abro el nuevo línea con Cecilia, a ver qué es lo que puedo usar. que esa es la línea de Cecilia supongo que esa va a tardar un poco más y posiblemente se monte en la caracterización de macro DR primero bien_
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 11:26 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 11:33 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 11:34 - Luciano Moffatt: _Bueno, entonces vamos a cerrar ya todo esto. Punto 1. Hacer correr lo antes posible el modelo 16 o como llamarlo. Punto 2. Caracterizar macro DR. Y bueno, ya está. punto 3 sería cumulative evidence, yo creo y punto 4 es correrlo en clusters, en Ecosamae es en GPUs, macro GPU, macro R GPU Ah, punto 5 Ah, ahí está, ese es más importante Antes que Macro R GPU Es Global Macro R Que sería poder optimizar al mismo tiempo Otro tipo de datos Que son datos de equilibrio Es decir, yo voy a manejar experimentos más complicados que tengan información cinética de distintas fuentes. Pero eso lo voy a hacer posiblemente SSA y también de canales únicos. Eso es el todo tema. Yo creo que, digamos, ositos y canales únicos serían las dos cosas que quedarían. O sea, que sería lo que probablemente termine trabajando con Cecilia Boussat. Hay mutantes Mutantes desde el punto de vista Teórico Hay muchas cosas_
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 11:34 - Luciano Moffatt: _Bueno, vamos entonces con un resumen de los... O sea, me quedan entonces dos papers a trabajar de inmediato. Este, que son... Bueno, acá continuamos. Estaba diciendo que ya tenemos dos papers definidos. O sea, digamos, el objetivo de dos papers. Él, a continuación, con Gustavo Zuno. Uno, ya me pongo a trabajar en ese modelo, y la caracterización de macro DR es el otro. Bien, una vez que termino esos, ¿cuál es el más importante que más cosas nos puede dar? Bueno, tengo varias cosas para analizar. O sea si lo planteamos desde el punto de vista de objetivos bueno ser poder digamos entender experimentos que involucren mutaciones Por un lado experimentos que sean en ositos es decir que sean m lentos por otro lado y experimentos que sean de canal sería la tercera cosa, que es hacer un poco lo más difícil y complejo, y lo de canal único yo creo que eso tengo que hacer una colaboración con más gente para llegar a un punto, probablemente con Lorin y algún otro más, posiblemente con este, claro, tenga que volver con la gente de Cojum, por lo menos de trabajarlo. de trabajarlo, eso yo creo que es importante, creo que de todos los problemas, o sea, volver a canales únicos para definir cuál es la likelihood con el tema de la idealización, eso ser un golazo o sea el tema con canales claro que hay el programa en realidad es conforme al modo de no sea es conforme al model m claro conforme al model con evidencia y aplicado a a ositos corrientes con ositos con mutaciones también y luego con formation model y evidencia aplicado a canales únicos a corrientes con un canal único ese creo que es el más bueno, no es tan difícil porque en realidad la pregunta es cómo se puede adaptar los caros cómo se pueden adaptar las idealizaciones el tema con la idealización es que vos ten una idealizaci pero desech otras idealizaciones posibles tom la tom la que tiene m la de clic pero no no tomas las otras y este y ah est un poco claro y vos tenés un error en los parámetros pero no tenés un error en los la de clic o sea con eso lo que habría que trabajar es como vos de alguna manera estás estás conseguí hablando la la iclijudo sam eso es todo un tema habría que plantearlo y ver cómo se puede solucionar pero el tema de canales únicos es un tema teórico muy importante muy interesante que es fundamental abordar y liquidar de una vez sí_
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 11:36 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 11:37 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 11:39 - Luciano Moffatt: _entonces cómo plantear el tema de la likelihood de modelos idealizados de canales iónicos me parece que yo me acuerdo que Cojum decía que no se podía hablar de likelihood, yo decía ¿por qué dice que no? y es por eso, quizás lo podría hacer con David directamente no estaría mal, habría que ver un poco cómo lo plantean ellos Ese es un problema teórico que podría ya mismo ponerme a trabajar, porque la idea no es solucionar el problema computacionalmente, sino por ahí teóricamente involucrar a otros investigadores importantes en eso, antes de plantear una resolución algorítmica quizás. Y el otro problema sí es más fácil, el de conformational model más conformational model con corrientes, con los hitos. Eso sí. Bueno, también vos podés, qué sé yo, con canales únicos ver cosas como la conductancia o ver un poco... Claro, pasa que igual, el tema es que con la duración de las aperturas y clausuras, el tema con las idealizaciones es complejo, hay que solucionarlo de alguna manera, no es tan fácil._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 11:39 - Luciano Moffatt: _Bueno, los otros dos problemas que habría que plantear es, bueno, una es el efecto del potencial de membrana y otra es la desinsitización. Sí, y bueno, esos son otros problemas que habría que estudiar dedicadamente._
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 11:42 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 11:55 - Luciano Moffatt: _bueno una de las preguntas es cómo me organizo no si yo tengo distintas carpetas para cada uno de los proyectos distintas como se dice esto ramas branches para cada uno de sus proyectos ramas de macro de r yo creo que eso estaría bien las ramas creo que sí tengo que abrir las distintas carpetas para cada uno de los papers, proyectos. El que me queda totalmente atrás es el de la programación con tipos, typas, proposition, en programing, eso es como que me queda un poco alejado. Y eso más que nada porque no tengo una comunidad alca a la cual rendirle cuentas. Entonces, digamos, en este momento no tiene sentido apuntar a eso, porque no tengo esa comunidad pero pero bueno quizás el año que viene si me pueda poner con eso_
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 17:05 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 17:05 - Luciano Moffatt: _bueno acá tengo otra sesión de audios esta vez en casa estoy en la pieza de candela bueno me acordé porque es que me puse a trabajar con macro r en lugar de continuar haciendo simulaciones el tema es que hay una cosa que está mal que es que la La distribución a posteriori de uno de los parámetros del modelo, el parámetro de inactivación, tenía una distribución muy extraña, digamos, cortada en creo que era 10 a la menos 4 y 10 a la menos 5, una cosa así. y generaba modelos en los cuales no hab directamente inactivaci lo cual era un tanto absurdo porque hay inactivaci se ve en la disminuci de la corriente Bien eso me hace desconfiar un poco de los modelos o sea que los modelos esos no estarían funcionando del todo bien, y habría que ver exactamente qué es lo que pasa con eso. El tema es que reproducir ese error es un tanto complejo, complejo porque aparece recién luego de muchas muchos días entonces me agarró como una especie de pánico de como reproducir ese error, yo digo bueno ya ahora entonces me pongo y trato de reproducir todos los errores o sea trato de ir desde abajo hacia arriba ir verificando el código como una actividad anterior a hacer una corrida ciegas que voy a tener que volver a hacer bien qu puedo hacer en realidad este si uno lo piensa como costo computacional correr el modelo s y después bueno por tener que correrlo de vuelta este no sería tan grave en cuanto a que si yo digamos sigo con el plan que había planteado esta mañana está así esta mañana fue este que es lo peor que puede pasar bueno que corra este que de repente bueno muestre o no muestre digamos una este fenómeno de de qué él la inactivación está presente o no lo que sea y este que él como se dice y que encuentre que el modelo funciona o no funciona o sea realmente es un poco independiente hasta cierto punto de eso digamos o sea la informaci que yo obtendr respecto de si estos esquemas que tienen acoplamiento en la rotaci digamos, la rotación, rotación, rotación, el acoplamiento de rotación, rotación, si tienen un efecto fuerte o no, es una pregunta importante para resolver, y creo que resolverla lo antes posible estaría bueno. Entonces realmente creo que no veo una razón como para no hacer eso. Yo creo que tendría que seguir con mi plan original, lanzar eso, lanzar esas corridas y luego ponerme a trabajar sobre Macro R. creo que el programa que estaba haciendo está bien, no no creo que esté mal bueno, vamos haciendo entonces ahora, bueno esto esto lo freno_
 -- Transcrito por zapia.com, tu IA personal
27/8/25, 17:10 - Luciano Moffatt: <Multimedia omitido>
27/8/25, 17:15 - Luciano Moffatt: _Entonces, ¿cómo sigo con esto? Bueno, entonces pensemos ahora sí en macroDR o macroIR, que sería el nuevo programa, digamos, qué cambios son necesarios y en qué orden, ¿no es cierto? Entonces quizás lo mejor sea tratar de reproducir cómo está Macro R, o sea con la nueva línea de comandos, y luego ir agregando las distintas mejoras. entonces cuáles son estas mejoras vamos a decirlas las mejoras a macro DR que esto va a ser una de las listas que tiene que sacar vos chachipiti de acá entonces las mejoras son bueno, una mejora la mejora más importante es desde ya hacer, digamos poder hacer programaci tipo no s pero que digamos que en la definici de la funci est incluida el test de esta funci es decir tengo que poder indicar a la l de comandos cuál es el test para cada función, ¿no es cierto? Bueno, esa es la primera cosa. La segunda cosa importante es poder salvar las variables, o sea, digamos, poder salvar lo que sería el environment, ¿no? El environment como un JSON, como ese formato JSON, cosa que yo pueda salvar y volver a correr, digamos, el programa. O sea, de poder, digamos, interrumpir un script y poder continuarlo. Luego, eso del script tendría que poderse interrumpir en medio de una función, ¿no? O sea, que específicamente en el caso de la función evidencia o la función sampling, que yo pueda interrumpir el sampling y despu retomarlo Y eso hacerlo desde una funci json ser excelente Bien despu la otra mejora muy importante es hacer una clase abstracta para modelos y para estas cosas de manera de que no... modelos y algoritmos, etcétera, de manera que no haya una una explosión de tipos templados, o sea, que disminuye esa explosión. Ese es otro de los objetivos importantes y bueno y eso sería y bueno y después este poder hacer una forma de correr digamos de lo que son los scripts para correr en la nube de correrlos de una manera que sea piola no sea este probablemente no s hacer un equipo ah quiz hasta no s si que macro de R genere los scripts o m bien generarlos desde un programa en R mismo que los genere. Eso tendría que pensar un poco cómo sería la integración, o sea, cómo generar los scripts o si los scripts los hago a mano y después lo veo de manera de automatizarlo. Eso es un problema. me parece que cuando llegue lo trataré de manejar. Después, la otra cosa, por supuesto, que tiene que ser, es ser un tanto ordenado en cuanto al ciclo de issues, el ciclo de documentación de cada paso que hago con la evolución de MacroDR, de tal manera que eso sea transparente, o sea, trabajar sobre la transparencia del código, cosa que yo lo pueda después publicar sin problemas, o sea, eso también sería otro objetivo muy importante._
 -- Transcrito por zapia.com, tu IA personal
2/9/25, 10:26 - Luciano Moffatt: <Multimedia omitido>
2/9/25, 10:26 - Luciano Moffatt: _Bueno, ahora tengo que seguir con macro DR. Entonces, la conclusión es que tengo que hacer un test para validar la dualidad de distribución sampleado. O probabilidad sampleado o probabilidad simulación. Bueno, esa adjunción categórica, estuve mucho, como una semana pensando en eso. bueno, llegué a la conclusión que sí, que es el adjunto pero claro, el adjunto es entre la digamos, que vos partís en un sentido va desde que vos tenés la función de probabilidad y la distribución la distribuci que cada conjunto de par le asigna una probabilidad y al rev lo mismo O sea ten una distribuci de probabilidad y a esa distribuci de probabilidad le adjudic un conjunto de par O sea, es más o menos trivial, pero bueno, es así como funciona. las disfunciones, o sea, vos encontrás que hay una biyección, pero bueno, ¿cómo llegás a la biyección? Ni idea, ¿no? Pero bueno, demostrás que existen, esa sería un poco la idea. Pero bueno, eso, más allá de todo eso, no, eso no nos serviría, en principio sí, como base, pero bueno, yo, digamos, tengo una idea que firme muy fácil que es muy fácil tiene inconveniente que tenés que desarrollar una herramienta más que es la derivada donde obtener el score, es decir la derivada de la likelihood y la ficha de information matrix o sea la derivada segunda o sea la derivada del score La derivada de likelihood y la derivada del score Y entonces usar la propiedad de que la esperanza del score es cero y que la varianza del score es igual a menos la esperanza de la Fisher Information Matrix. y probar esas dos igualdades como test, como un test de validez de la distribución. El tema con eso es que con eso, digamos, eso es válido claramente para una distribución multinormal, Pero claro vos podr secuestrar a tu distribuci con una distribuci multinormal y cumplir esos preceptos me parece As me da la idea Con lo cual no ser un test suficiente pero No, un test necesario, pero no suficiente, algo así. O sea, vos para el otro lado no te va. O sea, te permite discriminar los test que fallan, pero que acierta no significa realmente que el sampleado sea el adecuado. Podría samplear una distribución multinormal con los parámetros de Fischer que corre con el máximo y con una aproximación, digamos, multinormal en la máxima likelihood de la distribución y debería dar, me da la impresión. Entonces hace falta un testeo más preciso para demostrar que el sampleado corresponde a la distribución._
 -- Transcrito por zapia.com, tu IA personal
2/9/25, 10:28 - Luciano Moffatt: <Multimedia omitido>
2/9/25, 10:30 - Luciano Moffatt: <Multimedia omitido>
2/9/25, 10:31 - Luciano Moffatt: Eliminaste este mensaje.
2/9/25, 10:31 - Luciano Moffatt: _Bueno, y acá lo que yo siempre pensé es que uno puede aproximar cualquier distribución como una, justamente, una multinomial, ¿no? O sea, donde vos, digamos, hacés como una especie de aproximación, si querés llamar de Euler, donde generas bloquecitos cubitos de un volumen igual, suponete, de probabilidad. y entonces ver que esos mapean con tus ampliados y tenés ahí una distribución, siguen una distribución multinomial. Digamos, ese sería el otro test. ¿Cuál es el problema de ese test? Es que justamente necesitas, así como en el otro necesitas calcular el score, Acá vos lo que necesitas es calcular la integral, entre comillas, de la función del eléctrico, que no tiene una solución general. Esa integral es de alguna manera imposible._
 -- Transcrito por zapia.com, tu IA personal
2/9/25, 10:31 - Luciano Moffatt: _entonces acá la idea es supongamos que yo tengo que tomo una muestra de una distribución una muestra grande de 10.000 números no sé lo que sea bueno entonces claro yo ahora puedo hacer una puedo calcular de cada uno de esos 10.000 números la likelihood de cada uno de los puntos y luego lo que puedo hacer es calcular la distancia promedio de cada uno de esos puntos a todos los otros puntos de la distribución. Y entonces tratar de, digamos, teselar de alguna manera este volumen, ¿no? y entonces de alguna manera nada calcular lo que sería la integral de eso o sea, si lo teselas y bueno, calculas que cada cubito tiene la probabilidad de la sample entonces o podés hacer algo un poquito más sofisticado bueno, podés calcular directamente lo que sería entre comillas la evidencia que debería darte uno sí eso es una una forma de de testear la la el sampleado_
 -- Transcrito por zapia.com, tu IA personal
4/9/25, 19:49 - Luciano Moffatt: <Multimedia omitido>
4/9/25, 19:50 - Luciano Moffatt: _Bueno, vamos a hacer un resumen de todo lo que avancé en macro IR antes de ayer, ayer y hoy. Bueno, punto 1, el tema de las distribuciones. Bueno, encontré usando ChargyPT que hay varios test para, por ejemplo, hay un test que puedo hacer que está basado en una combinación del score con no sé qué cosa que incluye kernels gaussianos, en el cual puedo testear si una muestra es de una distribución. Después puedo testear si dos distribuciones son la misma y lo que me faltaría para completar el trío es obtener una likelihood a partir de una distribución. Es decir, yo tengo una distribución, quiero decir, tengo una muestra, no una distribución, y tengo un punto y que me estimen la likelihood de ese punto. Esas serían las herramientas con las cuales podría testear todas las cosas que necesito, que es ver si la muestra de una likelihood es correcta, o sea, si el sampling es correcto y despu si a partir del sampling puedo reconstruir los par que produjeron ese sampling Bueno eso por un lado Por otro lado estuve trabajando con el tema de los modelos Agregue dos modelos más, el modelo 16 y el 17, que incluyen el acoplamiento ternario, además del acoplamiento RR. Y voy a resucitar el modelo 8, que no lo probé, porque también puede ser que funcione bien. Estuve viendo que el esquema 16, como lo había planteado, no tenía bien los modelos estándar, es decir, no estaba incluido el acoplamiento RR para el prior de lo que sería la rotation, con lo cual, digamos, esas corridas que hice, bueno, habrá que tirarlas y considerar las corridas nuevas. De todos modos, no creo que cambie mucho, pero puede cambiar un poco, sí. Por otro lado, estuve viendo que no llega demasiado al equilibrio, o sea, no recupera bien la constante de, ¿cómo se llama esto? De caimiento, me sale, que viene a ser como la constante de inactivación, la ecotación y la creación no están dando bien eso es lo que tengo que ver más o menos urgentemente_
 -- Transcrito por zapia.com, tu IA personal
4/9/25, 19:51 - Luciano Moffatt: <Multimedia omitido>
4/9/25, 19:52 - Luciano Moffatt: _Bueno, y por último estuve tratando de ver si los modelos con el oscilador armónico para la rotación podían conducir a algo, y la verdad es que, no sé, para mí fue una burbuja eso, una pompa de jabón, no encontré ningún modelo deducido de ahí que sea diferente a los modelos marcovianos que estaba usando, es más, los modelos marcovianos me parece que representan mejor el paisaje de energía con el cual se entienden las reacciones químicas, así que me parece que eso es un dead end, no veo que eso conduzca a nada nuevo. así que seguiré con este camino lo único que me quedaría como dos opciones es agregar un estado más en la rotación o sea que sea una rotación de tres estados un estado intermedio de la rotación cosa de poder modelar mejor el acoplamiento entre rotaciones y después ver el tema de si separo rotación de gating también y modelo explícitamente la gating lo que pasa es que cualquiera de esas dos opciones va a incluir muchos más estados en el modelo y eso yo creo que no va a ser medio prohibitivo así que yo no creo que eso debería ser para otro paper más_
 -- Transcrito por zapia.com, tu IA personal
4/9/25, 20:05 - Luciano Moffatt: <Multimedia omitido>
4/9/25, 20:05 - Luciano Moffatt: _Bueno, en definitiva, digamos, los próximos pasos a seguir son, por un lado, bueno, correr los esquemas estos que tengo que correr, o sea, cortar todas las simulaciones y empezar a correr las que tengo que correr. y por otro lado, bueno, implementar estos test que estuve viendo con con CHPT, así puedo, digamos, testear el load model y todas esas cosas bueno, eso sería más o menos pero claro, primero en realidad tendría que implementar load model y implementar todo lo que ya está funcionando en macro R y para después agregar los test_
 -- Transcrito por zapia.com, tu IA personal
8/9/25, 12:12 - Luciano Moffatt: <Multimedia omitido>
8/9/25, 12:13 - Luciano Moffatt: _Bueno, vamos a ver, cuento un poco en qué estoy con macro IR. Bueno, yo tengo corriendo algunos modelos ya, entonces ahora me puedo dedicar a volver a tener un macro IR que corra con nuevas sintaxis. o sea, un poco una sintaxis que me guste más y la cual pueda implementar las ideas de HOT de Homo-Topy Type Theory en la ¿cómo llaman? Command Line Interface de manera que bueno, qué sé yo que pueda estudiar esas ideas y ver a qué conducen y expresar, digamos, algunas ideas que me parecen lindas en código y sentirme contento con eso. Pero bueno, hay una diferencia. Una cosa es el placer de programar cosas lindas, otra cosa es el deber profesional de publicar y que me paguen un sueldo por eso. Esa tensión entre esas dos cosas. También la idea es que yo programo lindo y con eso que sea un programa que sea mejor, que cumpla mejor esas funciones que yo digo, de ser más entendible, que otros lo puedan usar fundamentalmente, y que otros lo puedan usar, y que corran más que nada, más que que otros lo puedan usar, otros lo van a poder usar en tal medida que sea eficiente para hacer las tareas para las cuales lo queremos usar. Entonces lo primero que habría que definir es qué es lo que quiero usar, y bueno, lo que quiero hacer es usarlo para poder sacar información cinética de corrientes, de macrocorrientes y también de set de resultados experimentales. O sea, que yo tenga, o sea, eso son dos cosas, una es macro IR y otra cosa es Luthier, ¿no? O sea, macro IR lo que se especializaría en obtener, en hacer modelos cinéticos y bueno, y tratar de modelos cinéticos los cuales, digamos, un macro IR tiene que ser el algoritmo macro IR que permite obtener información cinética de fluctuaciones de macrocorrientes, los cuales son de canales iónicos, pero podrían ser también de motores moleculares. Bueno, eso es todo una pregunta, ¿no? Si me voy a meter en motores moleculares y todo. Pero bueno, van viendo poco para atrás. En principio tengo que poder sacar eficientemente de macrocorrientes informaci cin de una manera que sea robusta y confiable y que al mismo tiempo digamos nos ilumine en aspectos del mecanismo de activación de los canales no se es un poco el objetivo sólo logré con creces en el paper que estamos publicando en comunica y ahí lo logro bien, pero bueno, el tema es que las corridas son muy largas, o sea que el tema aquí es fundamental, sería que las corridas sean un poco más rápidas, eso me parece que es lo más importante, y yo me estoy distrayendo mucho en el tema de la belleza o comprensión del programa, Pero eso tiene que ver porque el programa ya tenía una complejidad que era difícil de manejar, entonces un poco lo tengo que domar y poder llevar a algo que sea interpretable, comprensible para otros y que pueda ser refactorizado más o menos rápido. O sea, la idea es que modularizar un poco el programa de manera que se pueda, digamos, razonar acerca de él de una manera más o menos fácil, ¿no? Porque ahora es un poco complejo. No me cabe a mí, ni a mí en la cabeza. Tiene un montón de secciones del programa, bueno, que son zombies y, bueno, tengo que eliminar los zombies. O sea, para poder eliminar los zombies y tener un programa mínimo que pueda realizar las tareas que tiene que realizar, bueno, para eso lo primero que tengo que hacer es definir test que me permitan decir, bueno, este programa que yo lo cambié, le saqué esta parte que me parece que estaba al pedo, sigue funcionando. y además un test que me diga qué tan rápido funciona, es decir, que tenga una idea de la eficiencia. O sea, que yo pueda medir del programa si cumple sus objetivos y con qué eficiencia. Eso es un poco el objetivo que tengo ahora a corto plazo, es decir, lograr que MacroIR me dé, yo pueda decir, bueno, yo corro, tengo esta versión de MacroIR, bueno, le corro estos test y me dice, bueno, funciona como venía funcionando y qué velocidad tiene. Entonces ahí yo ya puedo empezar a optimizarlo. O sea, ese es el objetivo. El objetivo tiene que ser exactamente ese. Lograr ahora, digamos, hacerlo funcionar de..._
8/9/25, 12:13 - Luciano Moffatt: _vuelta, porque en este momento no funciona, y tener separado una parte que es la l de la command line interface que ya tiene que ser digamos una tengo que tener un un domain specific language con ciertas funciones m y variables y todo ya definidas y más o menos rígidas y que permanentes tengo que tener eso y tengo que tener una mínima implementación con clases virtuales que después yo las clases concretas pueda variar de manera de que se optimicen o sea que sea llegar a una interfaz que sea más o menos estable y luego poder trabajar en la parte de implementación y buscar una implementación que vaya mejorando o sea que yo pueda mejorar y mantener la interfaz estable y tener una implementación que vaya siendo optimizada. Ese sería, es exactamente mi objetivo y donde la interfaz, y tengo que tener, digamos, más o menos separado las distintas responsabilidades con namespaces y directorios adecuados. Tengo, por ejemplo, la parte de los comandos. O sea, tengo varias cosas. Bueno, primero tengo que tener un lenguaje para armar los comandos. Y, digamos, tengo un propio lenguaje interno que me permita definir comandos, definir, digamos, sería como un lenguaje de programación de la Domain Specific Language. o sea, es una especie de metalinguaje. Después yo tengo el Domain Specific Language explicado a Macro IR, y ese Domain Specific Language va a tener una serie de clases, que serían clases virtuales, como el e-model, el e-experiment, y algoritmo, todas esas cosas, que después yo puedo ir agregando nuevas clases dentro de eso. En principio esas clases serían ya rígidas, es decir, para hacer una nueva tendría que recompilar todo, pero luego puedo llegar a pensar en una forma de construir yo, o sea, de un lenguaje que me permita construir nuevas funciones, etc. o sea que sea digamos m din Bueno esa ser como una segunda etapa La primera etapa es lo que tengo ya digamos que est dentro de este criterio de tener separados los dos o tres universos que serían, bueno, por un lado el universo de la implementación del Domain Specific Language, por otro lado el Domain Specific Language de Macro y R con los comandos específicos y las clases específicas, etc. Y después, bueno, ya sea la implementación de eso. Que esa implementación podría cambiar y podría ser que yo pueda, es más, seleccionar distintas implementaciones dentro de la parte más externa del programa, la superior, puedo elegir qué implementación uso, la implementación 1, 2, 3, 4, elegirla dinámicamente. Es más, tiene que ser así porque entonces ahí puedo, a ese nivel superior, comparar los distintos algoritmos. Yo tengo tres varios algoritmos, entonces tengo que poder, digamos, testearlos, o sea, medirlos con casos claros a nivel superior, a nivel de la ventana superior, cosa de tener un registro, digamos, comparable y registrable, documentable, de cómo uno va evolucionando la implementación. Es decir, que luego, digamos, siendo esa parte externa del programa más o menos estable, lo que va evolucionando es la parte interna. Esa es un poco la idea y que esa evolución sea toda documentada y bueno, cada tanto yo voy publicando la evolución de eso, ese es mi plan maestro de trabajo y es el que tengo que seguir, ¿no es cierto? Entonces ahora lo que estaba un poco preocupado era exactamente cómo integraba la parte de abajo del programa con la parte media, porque tendría una parte superior, que es la parte totalmente abstracta, la parte media, que son las cosas concretas, pero que pueden tener distintas implementaciones posibles y después la parte de implementación. Yo no sé si llamarlo, digamos, core and implementation. Como distintas, que podrían ser las implementations, podrían ser directamente cada una con un namespace separado, posiblemente._
 -- Transcrito por zapia.com, tu IA personal
8/9/25, 12:17 - Luciano Moffatt: <Multimedia omitido>
8/9/25, 12:20 - Luciano Moffatt: _otra cosa que estaba pensando en términos de facilities que yo voy a tener una facility por ejemplo para testing, o sea para definir los preconditions y postconditions y ver que eso se cumpla como hacer un domain specific language que permita expresar eso eso es una facility si querés otra facility que sea para salvar Otra facility es el environment Es como guardar variables Y recuperarlas Otra facility es como guardar el environment En el disco rígido y como cargarlo Otra facility puede ser como correr Programas en un cluster Otra facility puede ser Como elegir entre distintas implementaciones de un mismo algoritmo, una misma clase. Todas esas cosas están un poco interconectadas y claro uno puede tener una especie de mayordomo que elija digamos que optimice todas esas decisiones que vos digas bueno yo voy y tengo distintas implementaciones mido la eficiencia en términos de tiempos y lo que sea, y los recursos que tengo, entonces elijo lo que me conviene para determinadas circunstancias. esta sería como una facilidad de optimización también después tengo facilidades que tienen que ver ya más específicas en cuanto a bueno, modelo cinético facilidades que tienen que ver con experimentos otra que tiene que ver con distintos tipos de algoritmos y específicos algoritmos en general como digamos setear los parámetros de algoritmos y después bueno algoritmos específicos, algoritmos de simulación algoritmos de sampleado de evidencia de digamos de macro rs de macroscopic recursive likelihood serían un poco las distintas este facilitis o no sé cómo llamarlo que tiene macro de respuesta con que ver si alguna más que me falta_
 -- Transcrito por zapia.com, tu IA personal
18/9/25, 11:07 - Luciano Moffatt: <Multimedia omitido>
18/9/25, 11:13 - Luciano Moffatt: _Bueno, ayer me terminé de reconstituir MacroDR de manera que corra, que pueda correr una evidencia, una evidencia DTS. Y bueno, entonces me puse a pensar, bueno, ¿cuál es lo que tengo que hacer para generar un paper? Porque tengo que generar un paper rápido. y entonces me puse a pensar el tema este de hacer los test de cada uno de los comandos y claro, yo pensé, bueno, el test de la simulación, pero en realidad más elemental que la simulación son los modelos cinéticos, y entonces ahí me puse a pensar, bueno, cómo se hace un test de modelos cinéticos que es que vos tenés un conjunto de parámetros y que se relacionan con un conjunto de constantes cinéticas Y claro estar tambi si lo pensamos desde la de los reversos categ es decir las categor adjoins tendr que poder a partir de las constantes cin recuperar los par Pero claro no hay una biyecci porque vos generalmente lo que ten es que pod tener un conjunto de par cin que no correspondan a una aún unos parámetros y bueno ahí es donde entra toda la evidencia y el soporte de la eclips y hud porque vos digamos la relación en eso lo harías a través de predecir determinados experimentos Y entonces eso abre la posibilidad de plantear algún tipo de métrica, un espacio métrico de parámetros cinéticos y cómo mapean las constantes cinéticas a los parámetros cinéticos. y eso como ese mapeo depende de los experimentos eso sería un paper muy interesante para hacer me parece que estaría muy bueno y a partir de eso yo lo que pensé es que si lo simplificamos mucho nos quedamos en una unidad que ser por ejemplo un canal que se abre y se cierra con dos estados y m Y eso y claro uno que puede plantear es despu eso correr digamos frente a una corriente eléctrica, vos podés suponer que en realidad esos parámetros pueden ser dinámicos, pueden cambiar, y entonces vos es como que filtrás la corriente eléctrica a unos parámetros cinéticos. Eso sería como una forma de verlo también. Sería un modelo cinético alternativo y cómo eso se podría mapear a modelos cinéticos más complejos. O sea, eso sería una segunda etapa de esta idea de cómo mapear distintos modelos entre sí. O sea, cómo corresponde parámetros de un modelo con parámetros de otro modelo. o sea distribuciones a posteriori de un modelo con distribuciones a posteriori de otro modelo o sea Eso ser como un estudio que habr que hacer para Habr que afinar la teor de eso y con eso tratar de ver si podemos Yo puedo lograr algún tipo de algoritmo que sea más rápido, ¿no? Porque ahora el problema que tengo es que si yo tengo modelos alostéricos, tarda muchísimo tiempo en fitear, entonces tengo que encontrar maneras de idealizar los datos. Entonces lo que pensé, una forma de idealizar los datos sería esa, es decir, yo paso de una corriente a una serie de constantes cinéticas, y entonces después como que fiteo las constantes cinéticas, y después fiteo directamente la corriente sin la idealización en el medio, como la última etapa en un cumulative evidence. Esa es la visión que tengo en este momento, que sería, digamos, lo que habría que implementar, como una visión, entre comillas, definitiva para solucionar el tema de la cinética de canales diónicos._
 -- Transcrito por zapia.com, tu IA personal
18/9/25, 11:19 - Luciano Moffatt: <Multimedia omitido>
18/9/25, 11:20 - Luciano Moffatt: _Bueno, tengo que definir qué va a ser mi próximo paper. ¡Ah! Qué decisión difícil. Bueno, vamos a hablar de las distintas colaboraciones y distintos papers. Bueno, la colaboración de Cecilia Bousat está corriendo. Yo tendría que hacer una cosa muy parecida a la que usé en mi paper de Communications Biology, pero bueno, con un modelo que tenga cerrado, rotado, abierto y desensitizado, y todos acoplados alostéricamente, quizás sin asimetría o con asimetría, eso no sé, lo tendría que mirar, eso por un lado. Por el otro lado, el paper de, justamente, mirá, hablando, me hace arregla una de las cosas, tengo que revisitar el tema de la asimetría, es decir, ver el tema de la sincronicidad, o sea, de la interacción entre subunidades, ¿no? En apertura de canales con modelos alost y bueno ah yo ya veo que un modelo que incorpora ese acoplamiento deber ser mejor que uno que no lo incorpora y el tema que tengo es que las constantes de acoplamiento están demasiado dispersas, o sea, hay demasiado ruido en eso, yo tendría que ver cómo parametrizar eso de una manera que sea más elegante, ¿no? O sea, el próximo paso sería parametrizar eso un poco mejor. Eso es el paper 2. Y el paper 3 es, que sería uno que estaría yo solo, sería presentar macro DR en sociedad, ¿no? Y para eso no voy a hacer este asunto de desarrollo teórico, de la métrica de constantes sinéticas y todo eso, porque ya es demasiado. Pero bueno, sí hacer los test, quizás ni siquiera el test de sampleado. No sé, si el test de sampleado lo tengo que hacer porque tengo que saber que eso está bien. y posiblemente el test de los modelos también. Sí, los tengo que hacer. Tengo que hacer los test para todos los pasitos, todos los ladrillitos de MacroDR tienen que tener su test Eso es la presentaci del paper nada m que mostrar que todo est testeado con estajo Sí, no sé si presentar el homotopy type theory como en sociedad, digamos que sea un poco exagerado, pero sí el espíritu que esté ahí. O sea que ese sería una especie de paper. no, el paper lo que tiene que presentar es una domain diesel que se llama domain restricted language no sé qué es diesel tendría que presentar un diesel para escribir modelos cinéticos no, para escribir digamos experimentos experimentos mediciones de evidencia y bueno, nada, para presentar macro DR en sociedad, así como está, por lo menos que sea un poco más entendible que lo pueda usar alguien más o sea, lo presento así los, más que nada los test, ¿no? los test estos de como se llama, el FIM más que nada presento el FIM, que eso está bien y bueno, y veo lo que me dé la la nafta lo hago Pero sí, pero hoy empezaría posiblemente con modelos más simples. O sea empiezo con el modelo presentado en el paper de Biophysical Journal y despu presento un par de modelos m como los del paper pero bueno tratados de conseguirse claro, lo que tengo que hacer no, lo que tengo que presentar, perdón es la confusion matrix, eso es el output es el test de FIM FIM score ese por un lado para eso, para mostrar que como se dice que la aproximación de la aráctil Q es buena y eso tenemos que mostrar que macro NR falla y también tengo que ver las distintas formas de macro DR, ver eso como funciona, y eso por un lado y después la confusion matrix y la confusion matrix también tengo que hacerla con los dos y claro, y confusion matrix puedo tratar de hacer modelos más simples de dos o tres estados, una cosa así que sea más quizás más simple más fundamental Claro, eso sería como paso previo al siguiente paper, que sería el de la métrica, pero eso llegaría en un siguiente paper, quizás._
 -- Transcrito por zapia.com, tu IA personal
18/9/25, 11:25 - Luciano Moffatt: <Multimedia omitido>
18/9/25, 11:26 - Luciano Moffatt: _Bueno, se me ocurrió, digamos, qué hacer para que empiece a funcionar. Bueno, tengo que ya definir la primera figura. La primera figura va a ser el tema de la simulación. Bueno, tengo que hacer los dos test, ¿no? El test de los modelos y de la simulación. la pregunta que sigue es si ya hago modelos que sean digamos, hacer un domain specific language para modelos si eso es algo para hacer ahora la verdad que creo que sí porque si voy a hacer un test para eso lo debería hacer y además también lo que sea experimentos y modelos necesito hacer un Domain Specific Language para eso que ande bien y también para lo que sean corridas en el como se llama, en los clusters porque la idea es una vez que tenga todo eso poder empezar a correr experimentos en Tupac, en Tupac tenerlo todo el tiempo ocupado, aunque sea lento, digamos, o sea haciendo experimentos todo el tiempo y tengo que empezar a explorar, digamos, las distintas preguntas respecto de c optimizar la paga O sea en realidad no es optimizar El primer paso es como se dice caracterizar los distintos par que hacen a la medici de la evidencia y quizás no estaría mal la pregunta acá es vamos a implementar Cuevi y Levenberg Backward como alternativas para aumentar la velocidad de macro DR o vamos a limitarnos a lo que ya tenemos bueno, creo que en principio podría empezar con lo que ya tenemos que sabemos que funciona y una vez que eso ya empiece a estar caracterizado ahí sí pasar a Cuevi y después a Levenberg-Macquard también claro, Levenberg-Macquard sí lo tengo que hacer en realidad porque necesito las derivadas para los para los bueno, puedo hacer derivadas numéricas pero tengo que hacer la score versus el Fisher Information Matrix, eso yo necesito tener las derivadas, y eso es una parte fundamental del paper, así que eso sí tiene que estar, y después está el tema de query. Entonces tengo que ver ahí cuántos papers son, ese es uno de los temas que tengo que resolver ahora,_
 -- Transcrito por zapia.com, tu IA personal
18/9/25, 11:28 - Luciano Moffatt: <Multimedia omitido>
18/9/25, 11:30 - Luciano Moffatt: _o sea claramente huevi y le ven ver más tiene que ser separados o sea que eso no debería ponerlos pero puedo digamos tratar de hacerlos todos juntos cosas de sacar yo tres papers no sé estos todos al hilo uno detrás del otro eso podría ser este entonces sí pero bueno si tengo primero tengo que hacer una confusión matrix para modelos más o menos simples. Eso es el primer resultado importante. Pero antes que ese resultado sí tengo que hacer el Fisher Information Matrix para distintas aproximaciones de la likelihood. con distintas condiciones también de corriente etc y quiz tambi meter el tema del filtro de Beisel ah tambi Como que tengo que hacer dos partes El paper tendr dos partes Uno que sería Fischer Information Matrix como para caracterizar las distribuciones y después la Confission Matrix para presentar, para mostrar que las evidencias realmente sirven. la ficha de information matrix claro, con eso tendría que estar relacionado con las posteriori la distribución a posteriori y después la confusion matrix con las evidencias tendría que mostrar que las cosas están relacionadas porque la idea es que digamos, sí, tengo que un poco o sea, claro, el objetivo del paper es de alguna manera validar que las evidencias son a buena aproximación a la confusion matrix esa sería un poco la idea esa es la idea sí sí, sí, sí, totalmente_
 -- Transcrito por zapia.com, tu IA personal
18/9/25, 11:33 - Luciano Moffatt: <Multimedia omitido>
18/9/25, 11:35 - Luciano Moffatt: _A ver, entonces los conceptos que quiero tirar en este paper son cómo validar la likelihood y la evidencia. Entonces la likelihood la valido con la aproximación de la fin del score y con el posterior y el fin están relacionados. o sea, sí, sería ver algo así como mapeo de uno al otro y en la evidencia es la bueno, la evidencia y el confusion matrix sí la evidencia tendría que darte qué tan fácil es que vos mezcles uno con el otro sí, sí, sí la evidencia va a ser bastante más lío porque bueno tenés que no m o menos a ver c ser s claro s s ah con la evidencia habr que trabajar con un sistemita chiquitito a ver si uno puede realmente ver cuánto confundís. Claro, a partir de... Sí. o sea yo sampleo muchas veces entonces la evidencia como que la varianza de la evidencia no sé tengo que ver cómo se relaciona la varianza de la evidencia con qué carajo con con la hay varianza de la evidencia no, no, sí, hay eso sería interesante pensar cuál es la varianza de la evidencia bueno, sí vos tenés la varianza de la like la injustosa historieta eso tendría que ver un poco cómo es el tema ese la varianza de la evidencia interesante_
 -- Transcrito por zapia.com, tu IA personal
18/9/25, 11:36 - Luciano Moffatt: <Multimedia omitido>
18/9/25, 11:37 - Luciano Moffatt: _Claro, y lo que quedaría un poco afuera de este paper serían los modelos alostéricos. Ese sería el motivo de otro paper en el cual yo presentaría la Domain Specific Language para generar modelos alostéricos. eso tendría un paper aparte que claro, ese sí lo tendría que ver cómo hacer una plataforma para poder meter información de ositos o de otros orígenes eso no sé eso tengo que ver bien cómo hacerlo sí, porque si no, no sé bien cómo digamos, qué utilidad tiene esa plataforma, bueno, tiene utilidad para canales únicos, pero canales únicos no es algo que Cecilia Boussat maneje por ahora entonces estoy ahí un poco en una disyuntiva con el paper de cómo presentar los modelos arostéricos además el tema es que son muy pesados y necesito que sea un poco más rápido el análisis pero bueno, voy a tener que hacer rápido para el paper con Cecilia aunque bueno, si espero unos meses por ahí sale igual, no sé, tengo que ver eso_
 -- Transcrito por zapia.com, tu IA personal
18/9/25, 18:56 - Luciano Moffatt: <Multimedia omitido>
18/9/25, 18:57 - Luciano Moffatt: _Bueno, nuevamente en la 9 de julio caminando. Pensé varias cosas, vamos a ver si las puedo contar acá, en este lugar, en este espacio, en este espacio virtual que tengo. Me voy a putear por hablar al pedo más adelante cuando le dé esto sin que no dice nada. pero bueno, tenete paciencia la paciencia es la madre de todas las desgracias bueno, entonces vamos a ver qué es lo que pensé bueno, me metí un poco con el tema de la dualidad parámetros datos, que es uno de los temas grandes que me hicieron perder un montón de tiempo, estos dos meses no sé, que fueron de junio o julio, no me acuerdo la dualidad datos parámetros que traté de buscar bueno, como es una adjoin una operación de esas teorías de categorías, que yo creo que sí lo es pero bueno, la cuestión es presentarla y no hace falta formalizarla, quizás sí no sé, pero bueno, la dualidad como es, bueno, que vos tenés en realidad es un triángulo si querés, porque vos tenés datos, parámetros y un modelo, ¿no es cierto? es un modelo que te dice, vos le tirás los parámetros a un modelo y ese modelo te da una te samplea unos datos ¿no es cierto? y además te calcula una probabilidad entonces la onda es hacer el todo el problema es hacer la inferencia trata de bueno ahora dado que yo tengo los datos c obtengo los par que me dan esos datos es el problema inverso con una determinada probabilidad Entonces, digamos, o sea, todo el asunto es cómo yo puedo saber que mi operación de inversión del modelo es correcta. Esa sería un poco el problema teórico que habría que solucionar. O sea, cómo encontrar un test que te diga, bueno, si la operación que vos hiciste de inversión es la correcta. Bueno, una forma es encontrar efectivamente que los parámetros están dentro, que los parámetros recuperados se corresponden con los parámetros originales. O sea, que esto es como que la inversión es correcta. Y que bueno, vos tenés una prioridad y bueno, que esa prioridad se respeta. Es decir, vos haces muchas veces, aplicás, digamos, tenés parámetros, los parámetros fijos, ¿no es cierto? Generás una sample y recuperás los parámetros. Recuperás en realidad no unos parámetros, sino una distribución de parámetros. Bueno, ahora eso lo repetís muchas veces y entonces lo que vas a encontrar es que, digamos, como algo así, alguna operación en esas, digamos, diversas funciones recuperadas tiene que tener algún tipo de propiedad. Y esa es la propiedad que tendr que determinar exactamente cu es Si no vas a tener un conjunto de distribuciones a posteriori de los par y esa distribuci de distribuciones a posteriori, qué propiedad tiene, para que si el modelo es correcto, o sea, tendría que tener algún tipo de característica. Esa sería un poco la idea, ¿Qué característica debería tener? Esa sería una pregunta bastante clara que tengo que resolver. y bueno, o sea, no hace falta tener una respuesta definitiva de entrada, uno puede tener distintas aproximaciones, como por ejemplo, que los parámetros originales estén dentro de la distribución a posteriori, claro, que estén dentro, siempre va a estar dentro de la distribución a posteriori, sino que estén, digamos, en un caos, si vos lo repetís muchas veces, o sea, sería como una es un goodness of it finalmente esa sería la la resolución de eso es un problema que ya estaría solucionado en la estadística tradicional tendría que ver exactamente como se llama pero eso sería la resolución quizás esto no no sé si merece un paper en sí mismo o una buena explicación dentro del paper de caracterización de MacroDR ¿no es cierto? bueno s eso es claro despu tenemos el asunto de la esperanza del score que sea cero y el fin que sea igual a la varianza del score. eso es un es un dato que claro, eso no hace a la inferencia porque no tenés que recuperar la función, eso lo que te dice simplemente es claro, ahí lo que se está comparando es tu función de likelihood con las samples entonces, es decir, que el sampleado está bien, es correcto después la de inferencia es otro punto más, o sea que yo lo que lo que estoy claro, pues yo tengo..._
18/9/25, 18:57 - Luciano Moffatt: _samples y likelihood, o sea claro, esa es la original que la que sample y likelihood una es la, justamente es el add join o algo de la otra, y después lo que tengo es sample y inference que sería la la siguiente, ¿no es cierto? son dos relaciones que tengo que ver y eso sería tengo que definir bien esas dos relaciones, cómo las llamo cómo las caracterizo y ese es el paper, ¿no es cierto? o sea, hago eso y hago para un par de modelos nada más y ya está eso tiene que ser todo mi aporte en este paper_
 -- Transcrito por zapia.com, tu IA personal
18/9/25, 19:06 - Luciano Moffatt: <Multimedia omitido>
18/9/25, 19:12 - Luciano Moffatt: <Multimedia omitido>
18/9/25, 20:12 - Luciano Moffatt: _A ver, tratemos de plantear desde un punto de vista más o menos formal o práctico, cómo, digamos, vos podés obtener a partir de la likelihood el sampling y a partir del sampling el likelihood. Ok, entonces. Bien. claro, en realidad vos la idea, digamos, esta categoría categórica sería cómo vos podés comprobar que el que el likelihood corresponde a un sampling claro bueno, si vos tenés infinito sampling bueno, podés ver cada cubo n-dimensional y entonces ahí obtener la likelihood el tema es que eso es absolutamente como impractico o sea ahí vos lo reducís a alguna de esto suponete que vos podés vamos a suponer que vos podés claro de tu función de likelihood transformarla en una función ¿cómo se llama? no es de likelihood sería de claro una likelihood discreta no es decir que vos tenés vos ya no no producís datos continuos sino ten datos discretos es decir vos entreg puntos en un espacio n-dimensionado un espacio, digamos no n-dimension, sino finito entonces, claro vos ahí eso lo podés reducir a una distribución multinomial y ver si esa se ajusta, o sea como que lo reducís a un multinomio el otro enfoque es como decíamos el del film y el score y ahí medio que no yo eso ya le di miles de vueltas y no hay tutía es así bueno, ese es un punto el otro punto es otro que es que vos tenés un modelo y vos ahora proyectás vos tenías en realidad parámetros y samples, claro, podrías tener parámetros que de repente no afecten las samples, pero eso vos lo sacás con la derivada entonces ahí ya no hay drama claro si vos ahora bueno acá sería lo mismo vos con la fin tenés que saber si si, digamos tus datos te resuelven los parámetros lo que puede pasar es que tengas una topología medio compleja y tengas muchas porciones del espacio que sean equivalentes, ¿no es cierto? y eso es un problema muy difícil de de resolver eso es un problema como de topolog ese justamente es el problema m serio e importante de la inferencia Que es cómo manejarte en un sistema que es multimodal, ¿no? Que vos tenés muchas combinaciones que son más o menos equivalentes en cómo diferenciarlas unas de las otras. ese es el problema más más serio y ese problema sí, ahí hay que usar cosas de topología que no tengo mucha idea pero quizás y ahí depende claro, ahí tenés que tener algún conocimiento de tu modelo quizás no sea tan difícil quizás simplemente encontrar puntos derivadas o sea, mínimos, ceros de la la Fischer Matrix hay que pensar un poco eso quizás no sea imposible resolver, aunque claro lo que termina pasando es que tenés que claro, resolver un problema en más dimensiones en lugar de encontrar un máximo vos ahora vas a encontrar cero y esos ceros van a ser subespacios del manifold que separan los distintos modos. podr ver condiciones para que esos ceros no existan o sea c transformar un espacio que tiene simetr en algo que no las tiene Esa sería un poco una idea, ¿no? Me da interesante. algo así como llevarlo digamos sería como desanudar desenrollar un manifold no sé suena como algo que quería ver gente que trabajó pero yo no no sé No he visto nada al respecto. Es un problema mío difícil, digamos, que ver. Pero esa es una de las puntas de investigación más relevantes para acelerar los tiempos de esto. Pero, por otro lado, creo que la idea que, digamos, que se me ocurrió ayer o hoy, que es de simplificar los modelos como una suma de estados, de pares de estados aislados que después puedo poner a interactuar entre sí. Eso suena como una manera de acelerar los tiempos, ¿no? de, ¿cómo se llama? de cálculo_
 -- Transcrito por zapia.com, tu IA personal
18/9/25, 20:13 - Luciano Moffatt: _Bueno, pero lo que no terminé de decir es que, bueno, por un lado, digamos, el test que me dice, bueno, esta sampling, esta sampling proceder, esta sampling function y esta likelihood function corresponden a lo mismo. ese es el test del score y el film eso creamos como que no hay otra solución y por otro lado ah no, había otra solución sí, sí, sí, había encontrado que que podías hacer algo un poco más complejo ¿dónde estaba? eso sí lo tengo que buscar está en algún lado digamos, eso me ayuda Chachipiti que le podés no sé, poner otras otras funciones y entonces es como que podés explorar distintas partes del espacio con eso si, se lo tengo que ver bien pero bueno, en principio eso sería para una fase 2 ese estilo yo creo que por ahora en la fase 1 estamos bien con la ficha information matrix y el score y después la otra la inversión, o sea que un sampling y una funci de la X es la inversa de la otra Y eso lo resolv Y bueno, eso creo que es como que vos haces... Claro, no comparás con la identidad. Vos componés una likelihood con la otra y te tiene que dar, claro, justamente, una likelihood con la otra. ¿Qué tiene que dar? A ver. Tiene que dar que la esperanza de eso, del score de eso, sea cero, algo así, ¿no? claro, porque el score con el score yo sabía cuál era la original pero a ver cómo es el tema, porque si yo compongo una función con la inversa, me tengo que tiene que dar una especie de identidad pero no es una identidad porque porque pierde información o sea, no es que Si yo parto de una función delta, no me da una delta. Me da una delta convolucionada con algo. ¿No es cierto? Una delta convolucionada con algo Claro una delta convolucionada con algo qu entonces claro, esa es la pregunta, ¿con qué está convolucionada? bueno, está convolucionada con bueno, si vos lo aproximabas con una normal, sería como una normal ¿no es cierto? ¿Qué propiedades tiene esa distribución que vos convolucionás? Entonces, vos convolucionás dos distribuciones. Ah, y la distribución segunda, en realidad, no es una distribución cualquiera. No, no es arbitraria, no, tiene algunas características. Entonces, la pregunta es qué características tiene, ¿no es cierto? Claro. A vos, en algún punto, obviamente, sí. A ver, un caso típico, una normal con una media. Entonces, claro, vos la media, digamos, la perdés. O sea, vos amplías una normal, volvés para atrás y, claro, obviamente te desplazaste m o menos como una varianza Es como que todos normales Es como que ser como una especie de ...normal al cuadrado, pero no es normal al cuadrado. No sé qué es exactamente. Eso me suena como que tiene algún tipo de propiedad que es... es medio universal. Acá hay algo importante. Acá hay algo. Acá hay algo bastante bastante fuerte. Sí, bueno, con la normal lo podés sacar fácil, ¿no? Sí. Sí. O sea, vos igual tenés un prior, ¿no es cierto? Claro, vos partís de... Vos necesitas el prior para sacar la otra. Bueno, esa distribución la tengo que calcular. Porque claro, en el fondo, lo que voy a hacer es también ver si la sample sigue la distribución teórica. Bueno, el score sería 0, entonces tendría que ver eso. A ver, cómo es._
 -- Transcrito por zapia.com, tu IA personal
18/9/25, 20:43 - Luciano Moffatt: <Multimedia omitido>
18/9/25, 20:44 - Luciano Moffatt: _Otra cosa que pensé, que creo que no llegué a decir, es lo siguiente, que si vos tenés esta relación mutua de datos a parámetros y parámetros a datos, entonces si vos podés tener un prior para los parámetros que se transforma, ese prior vos se proyecta en los datos y a su vez vos podés retroproyectar el prior de los datos, es decir del rango de datos razonables al rango de parámetros que generan esos datos razonables y entonces ahí vos podés tener una especie de retro prior de los parámetros, que creo que lo hace alguna gente y claro, en realidad vos podés ahí ver una especie de evidencia a priori sería de un modelo en el sentido de que qué rango de parámetros del modelo dentro de el prior de parámetros posibles del modelo te generan condiciones compatibles con los experimentos o lo que sea Es decir es como una especie de experimento pensado si quer Es como una especie de cuantificación del experimento pensado. Es decir, un experimento donde vos planteás un modelo en el cual vos no tomás los experimentos, experimentos, pero bueno, vos simplemente decís, bueno, yo quiero, qué sé yo, un sistema que tenga determinadas condiciones, y esas condiciones vos las podés fijar casi sin experimentos, simplemente diciendo, bueno, tiene que, qué sé yo, un bicho tiene que sobrevivir tanto tiempo en promedio, ponerle una cosa así, entonces vos ahí vos podés hacer, digamos, podés cuantificar evidencias de modelos sin hacer experimentos. Una especie de, digamos, no sé cómo llamarlo, pero que podés avanzar bastante en modelos sin hacer experimentos, simplemente con ver qué tan razonables son las predicciones de un modelo._
 -- Transcrito por zapia.com, tu IA personal
20/9/25, 6:47 - Luciano Moffatt: <Multimedia omitido>
20/9/25, 6:49 - Luciano Moffatt: _Bueno, acá estoy en casa, sábado a la mañana, cantan los pajaritos, puse a trabajar sobre Macro.dr, ayer ya envié la versión final del manuscrito y contesté una cosa de sorado, así que estoy listo para trabajar sobre Macro.dr, estoy manejando sobre, estoy focalizado ahora main cpp. Bueno, quiero tener algo un poco más lindo, que es muy largo y confuso, pero más que nada lo que quiero ver es la estructura, que eso no es otra cosa que la estructura del programa, o sea, cómo el programa es percibido, cómo interactúa con el usuario. Entonces eso tiene que ver con el domain specific language que estoy exponiendo hacia el usuario. entonces cuál es la idea la idea es que en ese domain specific language o sea yo puedo correr scripts escritos en macro y r esos scripts tienen comandos o sea funcionarían como un espacio donde tenés variables que vos operas como si fuera un MATLAB, que vos tenés una serie de variables que tienen nombre, tenés una serie de funciones que_ _podés aplicar sobre esas variables, algunas de esas funciones escriben cosas, en principio por ahora algunas funciones podr correr cosas en la nube pero eso todav no est Eso por ahora lo haremos por afuera Pero entonces sí, lo que tenemos es, desde el punto de vista de macro DR, tenés un entorno de variables y un conjunto de comandos. En principio, en esta primera versión, el conjunto de comandos es estático, y lo que es dinámico son las variables. Entonces, desde el punto de vista del usuario externo, vos tendrías que poder acceder a cuál es la lista de comandos y cada comando debería tener algún tipo de especificación. Las especificaciones serían preconditions, postconditions, y bueno, las preconditions de cada uno de los argumentos de la función. O sea, la función tiene sus argumentos y el output. Y lo que tenemos también es un conjunto de tipos. O sea, quizá sería la tercera categoría. No tendríamos tipos, que los tipos son estáticos en este caso por_ _ahora. Tenemos tipos, funciones y variables. Esas serían las tres cosas. Y no tendríamos por ahora, o sí, funciones de rango superior. En realidad sí tendríamos la de test. Test y timing. Eso habría que pensarlo un poco. Tienen las dos funciones de rango superior que deber introducir Porque la idea es que nosotros queremos digamos es muy importante en esta etapa justamente todo esto apunta a este problema, a saber que el código corre como tiene que correr, es decir, que yo puedo hacer un test de cada uno de los comandos que yo planteo, para ver si ese comando pasa el test o no. Y por otro lado lo que tengo que poder ver es cuánto tiempo tarda en pasarlo, así yo puedo después optimizar el código alternando entre distintas implementaciones de los comandos, básicamente. Entonces, quizás, sí, bueno, uno puede, digamos, en principio alternar entre las implementaciones a partir de algún argumento del comando, ¿no? esa sería un poco quizás la forma estándar de_ _hacerlo bien, eso sería, a ver, algo más importante sí, bueno, y lo otro es, digamos ese es el, pensándolo macroDR como un lenguaje de programación de dominio específico Ahora si lo pensamos tambi como una cosa interact con el sistema operativo es decir que vos lo pod correr desde la l de comandos con ciertos argumentos y que te devuelve algo entonces ah la pregunta es te devuelve y ah es donde estoy hago un poco de agua en cuanto a cu ser las de los comandos cl para para esto no una cosa que yo ya intenté hacer era incluir con dos dashes, como está implementado ahora manacro.dr que corren los clusters, lo que hago es concatenar varias files uno arriba del otro y y por ahí en el medio le meto algún comando, o sea, lo que hace es, tiene como una función externa al DSL, que consiste en concatenar files, como si fuera un programa de C++ que vos haces includes, eso es lo que tengo ahora implementado entonces la pregunta es si eso tiene sentido o como yo, digamos,_...
20/9/25, 6:49 - Luciano Moffatt: 
_cuál es la forma clásica de la industria para manejar DSLs con la línea de comandos_
 -- Transcrito por zapia.com, tu IA personal

20/9/25, 7:04 - Luciano Moffatt: <Multimedia omitido>
20/9/25, 7:05 - Luciano Moffatt: _Claro, estaba pensando, como veo yo, digamos, como usuario externo, las operaciones internas de MacroDR, y bueno, lo veo a través del estado de las variables, pero las variables están adentro del programa, no accedo, salvo que las guarde. Entonces, claro, quizás lo que debamos tener es un guardado de las variables del environment a un archivo JSON o algo así que te guarde el estado del environment, de manera que justamente si se interrumpe el programa se pueda recomenzar. y además que pueda examinar que eso el JSON sea el output de alguna manera del programa_
 -- Transcrito por zapia.com, tu IA personal
20/9/25, 7:06 - Luciano Moffatt: <Multimedia omitido>
20/9/25, 7:06 - Luciano Moffatt: _Y entonces, claro, para saber qué puedo hacer con un macro DR en el help, me tiene que dar una lista de los comandos y también una lista de los tipos, es decir, los tipos de variables y la estructura de cada una de ellas. Eso sería una caracterización completa de macro DR. Y entonces yo lo que tengo que tener es un lenguaje de programación que permita manejar eso._
 -- Transcrito por zapia.com, tu IA personal
20/9/25, 12:05 - Luciano Moffatt: <Multimedia omitido>
20/9/25, 12:08 - Luciano Moffatt: <Multimedia omitido>
20/9/25, 12:11 - Luciano Moffatt: <Multimedia omitido>
20/9/25, 13:50 - Luciano Moffatt: _Ok, bueno, a ver, doy vueltas y vueltas con MacroDR y la IA me critica de que filosofo mucho o poco. Bueno, vamos a tratar de ser más prácticos. qué es lo que me está pasando, digamos, o sea, si me pongo a pensar, me pongo a rediseñar todo y ahí es una espiral, digamos, un descenso a las más profundas erocuraciones y distintas formas de extender el trabajo hasta el infinito que no se acaba nunca. Específicamente estaba pensando, bueno, el tema de cómo construir los modelos y verificar cada parte del modelo y tratar de expresarlo de una manera semántica y al mismo tiempo que testeable. Y eso es muy apasionante, muy lindo, pero bueno, lleva tiempo y yo no me decido hacerlo y el tiempo pasa y tengo que publicar. Entonces no voy a abrir esa parte del modelo, yo confío que anda más o menos bien. lo que sí tengo un poco de desconfianza es el tema bueno, no es que tengo desconfianza, lo tengo que probar es el tema de likelihood y el tema de la se llama la simulaci_...
20/9/25, 13:50 - Luciano Moffatt: 
_la simulaci digamos yo creo que es correcta lo que tengo que ver m que nada es el tema de que sea eficiente que el tiempo que lleva y comparar dos tipos de esto es lo importante, ahí está yo puedo hacer simulaciones basadas en la distribución multidimunomial o basadas en lo que quiero probar que es el, ¿cómo se llama? el Q delta T, la aproximación a la conductancia media y la varianza de la conductancia media. O sea, puedo usar eso directamente como output y hacerlo un poco más rápido. Entonces, claro, primero yo lo que tendría que hacer es verificar justamente que esa aproximación es correcta y una vez que yo verifico que esa es correcta, entonces la aplico directamente para, en realidad puedo aplicar las dos, las dos tipos de simulaciones, la simulación, digamos, segura de Taylor, lo puedo llamar de alguna manera, que yo considero que cada... para pasos muy cortitos, sin los cuales yo directamente..._
 -- Transcrito por zapia.com, tu IA personal

20/9/25, 13:50 - Luciano Moffatt: _lo que digo es tengo la simulación tipo Taylor que es decir hago deltas de tiempo muy cortitos y los simulo esos pasos muy cortitos y eso lo acumulo o hacer una simulación donde los pasos son los que corresponden y simulo la corriente tomando como verdadera la función de likelihood que poco. Lo malo de eso es que eso definitivamente no es verdad porque la distribución de la conductancia no sigue normal, sino que debería seguir una poasón o algo así, con lo cual eso no sería una buena estrategia. no no no definitivamente no debería usar eso sí yo creo que me parece que pero bueno podrías usarlo para para cuando vos tenés muchos canales ahí si no hay ningún problema entonces bueno no sé eso tengo que que bueno tengo que implementar ambos tipos de estimulaciones y este claro y ver cómo como digamos yo como las valido las simulaciones ese es el problema claro, yo lo que decía es una distribución de si lo que estoy validando es la multinomial ya está validada de_...
20/9/25, 13:50 - Luciano Moffatt: 
_por sí digamos sabemos que es así claro, si yo lo que hago es tomo pasos muy pequeños si ahí como es_
 -- Transcrito por zapia.com, tu IA personal

20/9/25, 13:50 - Luciano Moffatt: _la pregunta es cuál es mi simulación como se llamaría cold standard bueno, sería una tipo Taylor que yo digamos, lo tomo en pasos muy pequeños por ejemplo pero eso digamos cómo comparar dos simulaciones eso no me queda muy claro porque no digamos, sí lo que quedaría hacer es justamente o sea, compruebo la simulación con la distribución con la likelihood la likelihood de que likelihood justamente ese es el problema no tengo una likelihood verdadera porque sí, lo único que puedo tomar es la distribución normal de la corriente, pero no lo otro. Sí, lo que puedo comprobar es que justamente sí, que no era sino tomar la media y la varianza de las conductancias como o sea viendo que eso s esa distribuci o sea no me deber dar por el tema este de que se aparta de la normal, ¿no? eso es un tema que tengo que eso lo puedo analizar, o sea analizo, digamos, la likelihood de un solo paso, sería básicamente este sí podría haber o sea, como puede ser la conductancia ver los_...
20/9/25, 13:50 - Luciano Moffatt: 
_distintos estados eso sí, porque eso es para comprobar la simulación porque el output de la simulación es la distribución de estados no solamente la conductancia la corriente generada por el canal Eso es un output, digamos, sea output que uso, pero el output intermedio es la evolución del estado, de los estados, la prioridad, digamos, la distribución de estados. Claro. Gracias._
 -- Transcrito por zapia.com, tu IA personal

20/9/25, 13:54 - Luciano Moffatt: <Multimedia omitido>
20/9/25, 13:55 - Luciano Moffatt: _Bueno, sigamos. O sea, pensemos, ¿qué cosas son las que no confío del todo? Bueno, no confío del todo en, o sea, en realidad, digamos, podría tener motivos para no confiar en que la implementación de Jimin, Jibar y todo eso, En realidad, lo que tendría que ver más que confiar o no confiar es determinar en qué rango de condiciones eso funciona. Es decir, caracterizar tanto la descomposición espectral de la Q-matrix como el cálculo de la integral de E a la Qt, como m complejo este asunto de las la conductancia media y varianza de la conductancia media etc Eso tengo que validarlo todo, y más que validarlo es estudiar eso, en qué condiciones es correcto. Es decir, no sé, tendría que pensar un poco como parametrizar las cosas de tal manera de entender en qué condiciones es correcto o qué no. Una manera posible, fácil es tomar muestras aleatorias de los distintos modelos y entonces ver ahí en qué condiciones fallan, o sea, recoger todos los modelos que fallen, por_ _ejemplo, ver qué tienen en común o lo que sea. esa tendr que ser un poco mi estrategia en general o sea generar tener una forma de generar casos datos r y luego un test y ver donde el test falla. Para eso tengo que definir el test, bueno tengo que tener definida la función, tengo que tener definido el test y la forma de generar datos es fácil porque ya la tengo definida una especie de filtro eso sería una especie de función que tendría que definir quizás ese sería un test en realidad. En realidad tengo el test, es el test, que es decir, definir la postcondition y probarla y despu el test genera editor es tener bueno un sampler del modelo y generar una cantidad de samples y ver cu son las que fallan o son dudosas Esas serían las cosas que tengo que hacer con las cosas que son dudosas. dudosas, es decir, bueno, con la definición, digamos, de la, bueno, ya lo dije, ¿no? La composición espectral de Q, cálculo de P, cálculo de GB, min, GB, etc. y después el_...
20/9/25, 13:55 - Luciano Moffatt: 
_cálculo de la likelihood y la posterior distribución del posterior. Esas serían todas las cosas que tengo que probar. Definir los test para cada una de esas operaciones._
 -- Transcrito por zapia.com, tu IA personal

20/9/25, 14:07 - Luciano Moffatt: <Multimedia omitido>
20/9/25, 18:11 - Luciano Moffatt: <Multimedia omitido>
20/9/25, 18:12 - Luciano Moffatt: _La pregunta acá es ¿Qué es lo que estoy testeando? ¿No es cierto? o sea, lo que haría es saber para... o sea, tengo que definir una función, el nombre de una función, que yo la aplico a un modelo un modelo y un experimental step o un experimental concentration la función está definida, simplemente la tengo que exponer en el command line interface voy a empezar expandiendo cuánto 3 Gracias._
 -- Transcrito por zapia.com, tu IA personal
20/9/25, 18:12 - Luciano Moffatt: _Bueno, salí de casa para pensar un poco con Macro IR porque vengo hablando y hablando y no hago nada estoy pensando cómo reformular el DSL y no hago nada pero algunas cosas hice, por ejemplo llegué a la conclusión de que tengo que tener tipos y tengo que tener funciones y con el asunto de los nombres, claro Si yo tengo un tipo likelihood, tengo un calculate likelihood. Y si tengo un tipo probability transition probability, tengo que tener un calculate probability transition. Es bastante obvio. Y lo que pensé es el tema de cómo defino semánticamente los tipos. Y claro, desde el punto de vista de Homo teopita y theory, vos los tipos los definirías con su poscondición, un tipo o su invariante sería. O sea, los valores deben pertenecer a un subobjeto de este tipo. Hay algunos valores que son posibles, ¿no es cierto? Pero ahí yo me di cuenta que en realidad eso hace al objeto, digamos, computacional, que es importante, es lo único que vos usás para calcular. pero_ _existe otra informaci que es justamente la conexi entre el objeto computacional y objeto real o sea el mundo mundo real O sea si vos mediste un canal es un canal real o es una simulaci o de d viene ese dato O sea, que digamos, de alguna manera vos tenés que tener representado el mundo real. Y distinguir, digamos, las cosas que son, que pertenecen a la realidad, a un país, a una longitud, una hora, un punto en el espacio-tiempo, algo de nada, de una cosa que no lo es y que es un número aleatorio generado por un programa. Hay cosas que claramente son programas, los programas mismos también son generados por alguien, ¿no? O sea, alguien los inventó en un momento, o sea, fueron creados, o sea, que vos tenés un espacio-tiempo también de ellos, ¿no? O sea, todo tiene una atadura a un momento específico, ¿no? Y eso, digamos, el concepto este de la relación entre el programa y el mundo real, es ese tipo de abstracción que me gustaría a saber si está hecha y bueno_...
20/9/25, 18:12 - Luciano Moffatt: 
_tengo que ver ese punto y es un punto que es importante poder decirlo con precisión o sea separar lo que son relaciones entre los objetos computacionales dentro del programa y la relación con los objetos que no son que son del mundo real_
 -- Transcrito por zapia.com, tu IA personal

24/9/25, 14:12 - Luciano Moffatt: <Multimedia omitido>
24/9/25, 14:13 - Luciano Moffatt: _Bueno, anoche me quedé hasta las 2 de la mañana haciendo que compile el load model y luego que lo hice compilar y hoy tenía que dedicarme a entrelazarlo con Simulation y Evidence y todo eso un poco me detuve porque pensé si realmente valía la pena o si iba a ser más lento el programa, que es clave que sea rápido. Un poco también me detuvo la idea de que la compilación era bastante rápida, estaba en el orden de dos minutos, pero luego comprobé que simplemente lo que pasaba era que estaba compilando un solo modelo. Entonces, obviamente iba a ser rápido de esa manera. Pero bueno, también puedo trabajar con un solo modelo por ahora Y entonces el tema de integrar y modelar a toda mi disciplina de trabajo, mi flujo de programación, es quizás un poco inútil o no tiene un retorno al investment importante. Entonces, directamente pensé en ir a los bifes y buscar qué cosas tengo que hacer para avanzar más rápidamente a mi objetivo, que es publicar estos tres papers. Y_ _bueno, eso tiene que ver con convencerme de que las simulaciones que estoy haciendo son correctas, las simulaciones, las corridas de evidencia. Yo, digamos, lo que tengo la duda es con este asunto de la inactivation rate, que hay algo muy raro ahí. Entonces, para poder estudiar eso bien, quizás lo que tenga que hacer es exponer hacia la línea de comandos todo el manejo interno de macro DR. Cosa de poder probar uno a uno, cada uno de los componentes para ir probando distintas teorías. Es decir, que tendría que poner eso en la línea de comandos todos los objetos de macro de R. Lo cual implica que si los quiero exponer, los tengo que poder leer después en R para poder graficarlos o ver de qué se tratan. Y eso implica armar la forma de salvar los objetos de Macro.dr de una forma efectiva. yo pensaba hacerlo con JSON pero ya tengo un esquema que es el no me acuerdo como se llama tu data frame con lo cual quizás lo que tendría que evaluar es si eso con tu data_ _frame me alcanza para salvar la mayor parte de los objetos de macro DR o no Eso sería la pregunta que tengo que resolver ahora Porque si es así que los puedo hacer Puedo salvar y los puedo cargar también Entonces en ese caso me ahorro un paso Lo único que sí, bueno, tendría que tener el modelo y todo eso Y no tengo que hacer todo el JSON format que a la larga estaría bueno para hacerlo para una cuestión de que sea más transparente o que se entienda más pero bueno ya el tu data frame está implementado con lo que el camino más corto es hacerlo por tu data frame y bueno la idea sería simplemente empezar a troche y moche a generar nuevos comandos y exponerlos en línea working los la forma en que trabaja internamente macro de reacia la línea de comandos este eso como paso previo al testeo podr ser en paralelo las dos cosas al mismo tiempo a veces la cabeza así creo que tendría que hacerlo las dos cosas en paralelo bueno Bueno, punto y aparte, voy a contar una idea_ _que tuve bastante, bueno, la cuento acá en Macroderra, respecto del testeo de programas en general y el homoestepity theory, etc. La idea es, yo tengo una función, para que pertenezca a un tipo tiene que cumplir ciertos tests. Entonces, una de las cosas que siempre me preocupaba era, bueno, cómo, qué tan extensivos tenían que hacer los tests, etc. Entonces, lo que me di cuenta es que hay dos formas de testear. La función. Una es testear una implementación y ver que esa implementación cumple el test. Y eso, bueno. Ah, y lo importante es que esa implementación está referida a una distribución a priori de sus parámetros internos. O sea, por ejemplo, que se yo, de Evidence va a ser para un modelo determinado, un prior de modelo y un experimento, por ejemplo. O un prior de experimento, no sé. O sea que vos tendrías que tener un prior de sus parámetros. Y claro, si vos estás dentro de muestras de ese prior o de ese dominio, digamos, la función funcionaría. O sea_ _que vos probás una función para una especie de dominio. Bien. Bien. Eso por un lado. Entonces, vos tenés que en la triple o lo que sea que sea función, test de la función y dominio de inputs, vos...
24/9/25, 14:13 - Luciano Moffatt: ahí testeás esa función, ¿no? tiene un es valido una vez que mostras que es valido vos tenes otra implementación osea vos para ver que la implementación es estable lo que tenes que hacer es guardar los valores para esos parámetros determinados cuanto te dio y bueno, simplemente corres para esos valores y ves que es exactamente lo mismo, eso es para ver la estabilidad de la función. Entonces vos podés probar la estabilidad y lo podés probar con un número, digamos, finito de muestras, no hace falta que sean muchísimas. Y quizás, bueno, si después ves que fallan en algún punto, en algún test, Entonces, eso después lo guardás como punto de interés. Son puntos donde algunas interpretaciones fallan. Tenemos puntos que sean sensibles a errores. Esa sería un poco la idea._ _Tenemos un testeo de dos etapas. Un testeo primario y un testeo de seguimiento. Algo así. y la idea es hacer los testeos en la continuity integration, o sea, que de alguna manera se guarde esa información, de que esa implementación está compilada, y vos lo podés hacer, en realidad, cada vez que la compilás, lo validás frente a ese set de números, de variables. O sea que vos generás primero el set de lo que sería el patrón, y después te envías contra el test patrón. Y con eso detectas regresiones. O sea, vos tenés dos puntos, que una es la validación y después es seguimiento Y despu obviamente ten quiz una validaci a largo plazo porque en el seguimiento vos no est testeando valores individuales pero no estás testeando el test general, por ejemplo, si es distributiva o alguna otra cosa así de la función, que tiene que ver con la implementación esa pero bueno, quizás vos podés hacer un seguimiento después de hacer estos test más generales continuar haciéndolos,_ _o sea, que esa sería como la tercera parte, y ahí cuando eso falla guardar puntos de interés o digamos, hacer un reporte en lugares donde falla el programa, el algoritmo, lo que sea. Para tener una idea justamente del dominio donde el algoritmo da valores correctos. Porque justamente, exactamente eso. La función va a tener un dominio donde tiene valores correctos. No, realmente no. La función no funciona. Entonces, un poco la idea es determinar cuál es la región. del espacio de parámetros donde la función cumple con el contrato, con el posterior y las regiones. Posterior determinas las preconditions, o sea, las preconditions serían los valores del dominio que para esta implementación cumplen con las post conditions. Entonces, vos estás en el primer punto, es evaluar si vayan las preconditions, ¿no? ¿Qué área es? Y bueno, ahí también tenés el tema de que las postconditions pueden ser probabilísticas, es decir, que pueden tener determinadas intensidades, ¿no?_ _el tema este de velocidad de cálculo y precisión, ese es el otro tema que tengo que tener bien definido, y eso tengo que tener bien definidos los test, es decir, yo vuelvo al tema de que acá, para salir de este atolladero, de este agujero que no salgo, tengo que tener muy bien definidos los tests y las funciones, en el sentido de que yo, bueno, una vez que tengo una implementación de esa implementación de la función, tengo que tener, digamos, datos de, bueno, para para unas postconditions, para las preconditions que lo cumplen. Y claro, puedo tener distintas postconditions con distintas, digamos, potencias. Es una superficie de varias dimensiones, ¿no? Porque vos vas a tener las postconditions, vas a tener términos de precisión y de velocidad, y vas a tener las preconditions, valores que cumplen con eso o no lo cumplen. Claro, en realidad, digamos, vos, si vos, a ver, tomás valores de las preconditions en una sample, vos corres la función y después ves a qué_ _valores de la precondition se cumplen, ¿no? O sea, si vos tenés una función de alguna manera creciente, que tiene que ser de qué tan precisa es la función. O sea, si es preciso con 10, es preciso con 100 también. Es una función, digamos, trascendente o no sé. Es monótona la precisión. con respecto de los movimientos en el dominio de los parámetros de los argumentos, serian los parámetros...
24/9/25, 14:13 - Luciano Moffatt: y los argumentos de la función. Entonces vamos a tener una buena voz y aparte es lo que haces es que corres un n grande de argumentos de conjunto de argumentos, corres la función y después ya tenés el resultado, después lo pasas por la postcondition y bueno, ahí tenés argumentos a postcondition, ¿no? O sea, te queda una función que va de, en vez de argumentos al valor de salida de la función, tenés de argumentos a los valores de la postcondition. Y entonces ahí vos determinás eso y de esa superficie, de ese objeto matrimonético de lo que querés encontrar, que es, digamos,_ _algunas propiedades de tipo topológicas, es decir, por ejemplo, si todos los puntos que están en determinado radio tienen alguna cota o algo así. Es como un poco caracterizar ese objeto matemático que es las postconditions, el valor de las postconditions, que el valor de las postconditions puede ser un valor de dos dimensiones, que sea precisión y tiempo. entonces eso sería más o menos la idea del testing lo bueno es que planteado así el testing es una cosa totalmente en paralelo totalmente en paralelo, o sea que se puede hacer muy bien y bueno, vas acumulando información, ¿no es cierto? con lo cual, claro, es una cosa que podés ir acumulando información en las corridas en GitHub cada vez, ¿no? O sea, cada vez que corres, vas generando una salida más de eso. Está bien, estoy bastante, creo que está bastante bien lo que planteo. Y sí, es como un planteo muy general, se puede aplicar a mí a cualquier cosa. pero no todas las operaciones cada una de las funciones_ _y el macro de arra tiene que tener su test y sus condiciones y también su velocidad está muy bueno que se incendiera la velocidad también y claro, medirla en masa es fundamental Claro, para medir la velocidad también habría que ver qué cosas son esperables que cambien la velocidad del cálculo. No todas, hay cosas que ser más o menos constantes. Porque claro, Pero el tema es que también la velocidad tiene que repetir muchas veces lo mismo. Y eso, claro, no vas a repetir muchas veces lo mismo, los mismos valores me adoré. Es algo que sea una cosa aleatoria. Pero bueno, puede ser que haya funciones que uno ya sabe que no deberían cambiar. Vos tendrías a priori cosas que suponés que cambian con la velocidad y cosas que no. Ahí está, eso es un tema bien importante para manejarlo. O sea, el tema de qué argumentos a priori se supone que influyen en la velocidad de cálculo y qué argumentos se supone que no tienen ningún tipo de influencia, en lo cual se puede también_...
24/9/25, 14:13 - Luciano Moffatt: 
_testear. Bien, eso más o menos creo que sería todo. Ahora voy a pasar a la planificación del día._
 -- Transcrito por zapia.com, tu IA personal

24/9/25, 14:14 - Luciano Moffatt: Reflexiones sobre el desarrollo y testeo en Macro DR

- Estuve trabajando hasta tarde en el load model, pero me detuve a pensar si valía la pena integrarlo con Simulation y Evidence por temas de velocidad.
- Consideré enfocarme en lo esencial para publicar los papers, dudando sobre la inactivation rate.
- Para estudiar esto, necesito exponer el manejo interno de macro DR a la línea de comandos y poder leer los objetos en R para graficarlos.
- Estoy pensando en usar tu data frame para salvar los objetos de Macro.dr, ya que es un camino más corto que JSON.
- Tuve una idea sobre el testeo de programas en general, con dos formas de testear: una implementación y ver si cumple el test, y otra para ver la estabilidad de la función.
- Propongo un testeo de dos etapas: uno primario y otro de seguimiento, integrados en la continuity integration.
- La idea es determinar la región del espacio de parámetros donde la función cumple con el contrato, evaluando las preconditions.
- Para salir del atolladero, necesito tener muy bien definidos los tests y las funciones, con datos de postconditions y preconditions.
- El testing, planteado así, es totalmente en paralelo y se puede acumular información en las corridas en GitHub.
- Cada función en Macroderra debe tener su test, condiciones y velocidad, midiendo esta última en masa.
- Es importante identificar qué argumentos influyen en la velocidad de cálculo y cuáles no, para testearlos por separado.
- Ahora voy a pasar a la planificación del día.

-- Resumido por zapia.com, tu IA personal
24/9/25, 14:19 - Luciano Moffatt: <Multimedia omitido>
24/9/25, 14:21 - Luciano Moffatt: <Multimedia omitido>
24/9/25, 14:21 - Luciano Moffatt: _Un tema que me olvidaba es que vos tenés validación y seguimiento, y bueno, si seguimiento falla, tenés que hacer una nueva validación. Porque puede ser que, especialmente para funciones que dependen de números aleatorios, que algún orden de cosas te cambie los valores exactos del algoritmo. Entonces, en ese caso puede ser que vos tengas algún cambio en la implementación, que te cambie el valor que vos obtenés, pero que no deja de ser correcto. Entonces, en ese caso, sí, tenés que hacer una nueva validación._
 -- Transcrito por zapia.com, tu IA personal
24/9/25, 14:21 - Luciano Moffatt: _Y después el tema de la validación permanente y periódica y permanente es la posibilidad de que cada tantas funciones vos hagas una validación para ver que el algoritmo siga siendo correcto. correcto, más que nada para aumentar el área de la base del algoritmo, de que es correcto, a más regiones, porque vos podés haberlo validado para determinadas condiciones, pero querés extenderlo a condiciones poco probables, es decir, por ejemplo, cerca de la máxima lag, y jugó en puntos raros puede ser que el algoritmo falle de alguna manera entonces es bueno eso tener un registro para saber lo que pasa exactamente no_
 -- Transcrito por zapia.com, tu IA personal
25/9/25, 19:50 - Luciano Moffatt: <Multimedia omitido>
26/9/25, 11:18 - Luciano Moffatt: Eliminaste este mensaje.
26/9/25, 11:19 - Luciano Moffatt: _bueno acá estoy cagándome de frío afuera, no voy a entrar, bueno ya volvió Andrea bueno ayer y hoy, antes de ayer también tuve unas sesiones muy intensas con Codex avancé bastante, tengo ya tengo integración, ¿cómo se llama? es continuous integration con github tengo ya un día salvo en formato json las variables del enviroment y tengo un par de comandos nuevos que calculan creo que el patch state que viene a ser el patch state es la q 0 y y QA y G de un modelo Despu tiene calculo tambi los eigenvalues y la probabilidad de transici y no me acuerdo qu m Pero bueno la cuesti es que ya tengo algunas funciones de las cuales tengo que definir el famoso testing. O sea que tengo que armar el testing. La primera tendría que ser la función de aplicación de un modelo. El tema es que el test para esas postconditions dependen de cada modelo y es exactamente lo que define cada modelo, como veis la relación entre los parámetros y las constantes. Entonces, claro,_ _exactamente. Para cada modelo tengo que definir un testing, y decir, el testing es el que define el modelo de alguna manera. Hay una relación entre... S es bien claro que cada esquema tiene una biyecci a unas post conditions entre par y las matrices Q. Eso es un poco más complejo de lo que quisiera analizar ahora, pero bueno pero en realidad habría que ver eso exactamente o sea a ver en la circularidad yo tengo una función que va de parámetros a matrices Q y otra que va de matrices Q a parámetros van rematiscos a patch model entonces puedo ver exactamente esa circularidad para para ver que el modelo sea correcto creo que esa falla para algunos modelos alost pero bueno podr tratar de ver eso eso podr ser una una forma de de empezar con los test no sé que entonces ahí definiría una especie de claves un test para el objeto el modelo modelo y parámetros a ver, pero el objeto ahí sería ¿cuál es el objeto? el objeto es en los parámetros o sea, a ver es el modelo_...
26/9/25, 11:19 - Luciano Moffatt: 
_modelo, sí más los priors probablemente. Esclavo, definís más estas dos funciones. Ok, o sea que ahí definiría el primer objeto._
 -- Transcrito por zapia.com, tu IA personal

26/9/25, 11:19 - Luciano Moffatt: <Multimedia omitido>
26/9/25, 11:20 - Luciano Moffatt: _Bueno, vine para acá para contar algunas ideas y la verdad que tenía muchas ideas para contar. ¿Por qué tenía tantas ideas? Bueno, ocurrió que estoy llegando al punto donde finalmente voy a implementar la jotificación, ¿no? La homeotopy type theory, aplicación a macro IR, que es un poco algo que me viene ronroneando el cerebro desde hace bastante tiempo. Es como lo que más curiosidad y más pasión o interés me demanda, más magnetismo sobre mí. Bien entonces pens que hab muchos elementos separados para trabajar Lo primero que quer decir es que un poco quer en este punto retomar la planificaci de Macro IR y tener documentos que respalden esa planificaci Porque estuve trabajando mucho en código. En realidad lo que estuve haciendo es casi todo códex. Yo simplemente di las instrucciones. Y al mismo tiempo encontré, bueno, esto de la Continuous Integration con GitHub me parece que es una gran herramienta que me da como cierta credibilidad interna y externa también._...
26/9/25, 11:20 - Luciano Moffatt: 
_O sea, vos que tengas test que tienen que pasar, hacen que uno realmente pueda acometer refactorings importantes de macro IR con total desparpajo e impunidad. Muy bien, entonces, bueno, eso sería una pequeña introducción, ahora continúo._
 -- Transcrito por zapia.com, tu IA personal

26/9/25, 11:22 - Luciano Moffatt: <Multimedia omitido>
26/9/25, 11:22 - Luciano Moffatt: _Bueno, acá, entonces, un punto es que le voy a ordenar a... ¿cómo se llama este bicho? a Chachipitín que coordine, digamos, a un resumen así, un plan de acción de la documentación, de la documentación de la documentación de coordinar un buen documento para el plan de factoring de macro IR de lo que queda, a ver si puedo por todas ser tanto ordenado bien los elementos de factoring va a tener que ser toda la documentación que está por ahí dando vueltas y los issues porque una de las cosas que quiero hacer es definir entre otras cosas, como es mi manera de trabajar si voy a seguir usando los issues o no, yo creo que sí, tienen que estar los issues como una forma de documentación. Y bueno, entonces una de las cosas que tengo que hacer es ver todos los issues que ya están planteados y rellenarlos o concluirlos o mantenarlos o lo que sea, después de analizar todas las cosas que voy a analizar bueno, entonces yo tenía como varias ideas pensaba que podía cada una_...
26/9/25, 11:22 - Luciano Moffatt: 
_hacer algún tipo de audio así que vamos a ver, voy a hacer eso cada idea un audio bueno_
 -- Transcrito por zapia.com, tu IA personal

26/9/25, 11:24 - Luciano Moffatt: <Multimedia omitido>
26/9/25, 11:26 - Luciano Moffatt: <Multimedia omitido>
26/9/25, 11:29 - Luciano Moffatt: <Multimedia omitido>
26/9/25, 11:31 - Luciano Moffatt: <Multimedia omitido>
26/9/25, 11:33 - Luciano Moffatt: <Multimedia omitido>
26/9/25, 11:37 - Luciano Moffatt: <Multimedia omitido>
26/9/25, 11:54 - Luciano Moffatt: <Multimedia omitido>
26/9/25, 12:55 - Luciano Moffatt: 
_Un siguiente concepto que tengo que tener es poder expresar es el concepto de prior o de distribución. Entonces, ¿hay alguna diferencia entre prior y distribución? Bueno, en principio no. O sea, una distribución sería como un refinamiento de un dominio. Es un dominio al cual vos demuestras que a la vez tenés una relación de probabilidad. No sabés que era una muestra, sino a su vez podés de esa muestra tener un valor bien. Eso sería el punto 1, eso sería una distribución. Ahora, un prior es una distribución de la cual vos le asignás algún tipo de semanticidad, o sea, tiene algún significado, o sea, una distribución sería como un prior del cual vos no sabés nada a qué se refiere, lo único que sabés es el cociente de los prior, no de la distribución, de alguna manera. Podría pensarse así, o sea, sería la diferencia entre un prior y una distribución. O sea que una distribución, una prior sería una distribución más algún tipo de contexto, de significado o algo así._
 -- Transcrito por zapia.com, tu IA personal

26/9/25, 12:55 - Luciano Moffatt: _Esto nos lleva a la pregunta de qué es. ¿Qué puta es un significado? Eso sí que es. ¿Qué carajo es un significado? No tengo idea. ¿Qué es un significado? ¿Cómo podés de alguna manera expresar un significado? Bueno, en principio es lo que llaman las prioridades condicionales. condicionar a que determinadas cosas sean ciertas. Es decir, un prior es una distribución condicionada a determinada información. O sea que el significado sería la información que vos tenés respecto de cosas. O sea, es un contexto. Ahora, el contexto, ¿cómo sería? Y bueno, podría ser el enviromente en el cual estás. O sea, vos tenés un prior, sería como una distribución para un determinado envíromen, sería esa forma de plantearlo. No, es un enviroment, es que vos tenés un grupo de variables definidas y nada, digamos, si estás adentro de un enviroment, entonces, puta, si vos estás ahí adentro, significa que perteneces de alguna manera al grupo lo que sea al contexto y entonces ah pasar a_ _ser un priori Eso no s si tiene mucho sentido pero m o menos algo tiene Lo pod decir digamos o sea en realidad ac hay dos posibilidades es cierto Una es que vos tenés un environment, vos estás en un environment, o sea, te das en un lugar donde otras cosas están definidas, o sea, entonces, nada, vos en ese environment suponés, tenés ciertas cosas que sabés y entonces entre otras cosas serían estos priors porque la idea es que digamos, al envío metes en qué sentido tiene sentido en el sentido de que un prior no está aislado sino que está en conjunción con otros priors o sea, vos tenés un conjunto de conocimiento no es un solo conocimiento sino que es justamente vos tenés ahí está la prioridad condicional es condicionar a todo un contexto es decir que el contexto está dado en las otras prioridades condicionales es decir que hay un conjunto de conocimientos que uno se supone digamos que operan o que que se dan al mismo tiempo que suponen unos a los otros entonces_...
26/9/25, 12:55 - Luciano Moffatt: _Bueno, ahora entonces me viene el tema de objeto environment, que es un environment. Un environment es un lugar donde vos sabés cosas, o sea, vos tenés una determinada lista de variables que tienen valores definidos. Entonces, en principio sería como un conjunto, o sea, es un producto cartesiano de distintas variables. algo así, eso sería como un envíromen, y el objeto sería más o menos lo visto, pero al principio, el objeto de envíromen no me queda claro, pero quizás vos podrías decir que un envíromen pasa a ser objeto cuando se cumplen determinadas post conditions respecto a los elementos del environment. O sea, vos tenés un environment que tiene ciertos valores, ciertas variables, tenés ciertas funciones, podés tener funciones de funciones, etc. Y vos lo que tenés es que ciertas cosas rigen, ciertas igualdades rigen. hay ciertas postconditions de determinadas funciones es decir, a ver, ahora, ¿cómo es la cuestión? ahora tendría que definir un poco qué es_...
26/9/25, 12:55 - Luciano Moffatt: 
_postconditions_
 -- Transcrito por zapia.com, tu IA personal

26/9/25, 12:56 - Luciano Moffatt: _Bueno, las postconditions, en principio, como las disfunciones, son cosas que no serían, digamos, definidas para un determinado envíodo. No, vos lo que decís es, por ejemplo, una postcondition es la inversión, el ciclo, o la función inversa. una función compuesta por otra función te da la función identidad. Entonces vos ahí lo que necesitas son tres funciones, la inversa de la función y la función identidad, y una tercuarta función que es la función igualdad, entonces vos decís que la función compuesta con la función inversa es igual a la identidad. o sea que tendríamos otra más que sería la compuesta, la composición. Entonces eso ser una propiedad la propiedad de funci inversa o de ciclo no sé carajo se tenga que llamar y esa propiedad nada es digamos agnóstica respecto de los empiromen porque vos simplemente le das todas esas funciones que tiene que tener la función, la inmersa en la función, la identidad, la igualdad y la composición, y bueno, se tiene que_...
26/9/25, 12:56 - Luciano Moffatt: 
_cumplir eso. Ahora, entonces, vos lo que tenés es que si vos tenés un enviroment, y a ese enviroment le aplicás una serie de postconditions, sobre... ya no puedo entrar en la llamas entonces cumplen todas esas cosas se cumplen entonces ahí tendrías un objeto esa sería una posible definición de objeto_
 -- Transcrito por zapia.com, tu IA personal

26/9/25, 12:57 - Luciano Moffatt: _Y ahora vos lo que tenés son dos objetos que son idénticos en el sentido de que ambos cumplen esa poscondición. vos lo que podés tener ahora que sería digamos la base de teoremas o generalizaciones es que si vos podés tener que un objeto que sigue una poscondición por ahí compuesto o algo con otro objeto sigue otra poscondición y Sigue otra tercera poscondición que integra los dos objetos y lo que podés decir es que quizás vos generalizás que cualquier objeto que siga estas poscondiciones, la poscondición 1, después sigue la poscondición 2. Eso sería algo así como la base de los teoremas o el razonamiento matemático o lo que sea. Que vos lo probás en un lado y transportás la prueba a otro lado. Es decir, por ejemplo, si vos ponele para calcular la matriz de probabilidades de transición, necesitas el exponencial matriz y bueno y no importa vos digamos dos funciones que cumplan las pos condiciones de la exponencial matriz o sea pertenecen al tipo de ponencia_...
26/9/25, 12:57 - Luciano Moffatt: 
_entonces despu esas vos defin la claro definir la el tipo probabilidad de matriz, probabilidad de transición en base al tipo exponencial matriz. Entonces, nada, después lo tenés, o sea, como que vos aislás lo que sería la implementación del código. O sea, vos lo codificás una vez y después podés elegir cualquier implementación. Esa sería un poco la idea de esto, de usar funciones equivalentes y definir tipos de funciones, que en este caso serían, claro, objetos, ¿no? O sea, objetos de un determinado tipo. O sea, vos cómo definís que un objeto tiene determinado tipo? bueno, porque cumple cierta poscondición, vos tenés ahí eso, o sea que vos tenés determinadas, es un envirio, vos tenés un determinado conjunto de funciones y de variables que cumplen ciertas relaciones, y bueno, si cumple esas relaciones vos decís que pertenecen a determinado tipo. Es decir, que ciertos, como se dice, predicates son válidos. Lo cual, digamos, es totalmente así._
 -- Transcrito por zapia.com, tu IA personal

26/9/25, 12:57 - Luciano Moffatt: _Bueno, y esto de haber pensado, por ejemplo, la función inversa, que vos tenés una función, tenés la inversa de la función, después la función identidad, la función composición y la función igualdad. Si vos tenés funciones que sean estocásticas, es decir, vos tenés una función, ¿no es cierto?, que te manda una función de probabilidad. ¿No es cierto? Entonces, dado que vos tengas una función de probabilidad, vos obtenés los parámetros de la función de probabilidad. Entonces, claro, si vos lo que tenés es una muestra ahora, entonces vos tenés la muestra, vos tenés una función de probabilidad y tenés una muestra de esa función de probabilidad. Entonces vos lo que tenés ahora es una función, he dado una muestra de la función de probabilidad, te da los parámetros de esa función de probabilidad para generar esta muestra bien, entonces vos lo que vas a tener es que la distribución vos tenés una, no, vos lo que vas a tener es una función que es una distribución es_ _decir vamos por partes si vos tuvieras claro si vos ten una funci una distribuci entonces ahora vos ten la distribuci y a partir de la distribuci vos sac los par es cierto Es más o menos directo, ¿no? Es casi, digamos, es un dato ridículo. Vos tenés parámetro media y desviación estándar y generás una normal con esta media de división estándar. Entonces vos tenés una distribución normal con una media de división estándar y le extraés los parámetros. Es decir, digamos, es una biyección, digamos, trivial, pero bueno, igual son objetos matemáticos diferentes y vos los obtenés. la cosa un poco cambia, se pone un poco más interesante si vos, ahora vos tomás una muestra de esa distribución normal y tenés una determinada cantidad de samples entonces ahora a partir de esas samples vos lo que ibas a obtener es una muestra de una distribución, perdón, de los parámetros, ¿no? es lo que sería sí, eso, entonces vos lo que te dice, digamos, la regla esta, que si eso está_ _bien hecho, vos lo que te tiene que dar es que los parámetros tienen que estar, los parámetros originales, tienen que estar incluidos dentro de la distribución, o sea, la distribución de parámetros resultante deber perd lo que ser lo que vos ten es ser el F ser vos ten par y obten una muestra El F inverso es vos tenés la muestra y obtenés una distribución de parámetros. Y esa distribución de parámetros, lo que te va a dar es que los parámetros originales tienen un valor razonable dentro de esa distribución de parámetros. es decir, que si eso vos lo hacés muchas veces, nada, te va a dar un... En realidad lo que vas a tener es, o sea, vos a partir de, a ver cómo es esto, vos tenés una función de distribución y obtenés otra función de distribución que te genera parámetros, ¿no? O sea, vos parás de pasar parámetros a el espacio de muestras, de observaciones, ponéle. Y entonces, ahora vos lo que haces es, vas de parámetros a parámetros, volvés a parámetros y tenés_ _una muestra de parámetros. Entonces, ahí la pregunta, esa sería una muestra que debería ser, digamos, una muestra de identidad, o sea que debería que estar centrada en los parámetros originales de alguna manera, o no sé si centrada o que los contenga, con determinada varianza o esté un poco alrededor. O sea que vos y eso digamos ese alrededor digamos esa varianza deber depender del n de par que ten y probablemente dependa claro si vos haces una digamos simplificación de la FIM, de la Fisher Information Matrix, uno podría, digamos, hacer una aproximación normal de eso y ver qué da y después ver si eso lo podés generalizar para distribuciones que no sean la normal si vos la pudieras de una manera normalizar cualquier distribución vos la podés normalizar es decir, hacer un cambio de variables y llevarla a una distribución normal ¿es eso posible? principio creo que si vos para determinadas topologías calculo que sí porque es una función continua de un lado al_...
26/9/25, 12:57 - Luciano Moffatt: 
_otro debería darte bien o sea, si vos lo deformás en la distribución claro, ahí sería ahí está la cuestión, si vos sos una distribución continua que manda de un parámetro a otro, entonces es como que vos la normalizás, que es una transformación para normalizarlo. Entonces si vos suponés, todos vos lo podés decir, es bueno, lo pruebo para una distribución multinormal y después veo cómo es la parametrización para llevar a una normal y ahí demostrar el teorema este, a ver si eso existe._
 -- Transcrito por zapia.com, tu IA personal

